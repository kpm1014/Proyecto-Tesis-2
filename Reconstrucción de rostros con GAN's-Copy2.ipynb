{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473d4af4",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0dbcfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:31.126692Z",
     "start_time": "2022-08-24T03:57:31.107691Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798fa2bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T14:06:47.245954Z",
     "start_time": "2022-08-24T14:06:43.378918Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.datasets as dtst\n",
    "\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Retiramos los mensajes de alerta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d49cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:44.151701Z",
     "start_time": "2022-08-24T03:57:42.553780Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba7ef2",
   "metadata": {},
   "source": [
    "# Configuraciones para la tarjeta de video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53d15f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T14:06:47.308953Z",
     "start_time": "2022-08-24T14:06:47.295952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# El sistema selecciona el hardware disponible\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dae917d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:44.183702Z",
     "start_time": "2022-08-24T03:57:44.170703Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Ti', major=8, minor=6, total_memory=8191MB, multi_processor_count=38)\n"
     ]
    }
   ],
   "source": [
    "# Verificamos qué Hardware estamos usando\n",
    "if device == \"cuda:0\":\n",
    "    print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df127870",
   "metadata": {},
   "source": [
    "# Definimos el DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070df2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T14:06:54.175268Z",
     "start_time": "2022-08-24T14:06:52.673736Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_complete = './Images/original/'\n",
    "data_folder_pixeled = './Images/pixeled/'\n",
    "# Tamaño que deseamos que tengan las imágenes\n",
    "image_size = 32\n",
    "# Tamaño del lote de imágenes\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "dsimgs_1 = dtst.ImageFolder(\n",
    "    root=data_folder_complete,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # El problema encontrado es que necesitamos hallar la desviación estandar\n",
    "        # media de toda la información para realizar una correcta normalización\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "dsimgs_2 = dtst.ImageFolder(\n",
    "    root=data_folder_pixeled,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # Esta desviación estandar y media es la hallada para los datos de\n",
    "        # entrenamiento\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91cb37b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.328877Z",
     "start_time": "2022-08-24T03:57:45.314880Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_loader_real = DataLoader(dsimgs_1,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=4,\n",
    "                       drop_last=True)\n",
    "\n",
    "dt_loader_gen = DataLoader(dsimgs_2,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=4,\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea53080",
   "metadata": {},
   "source": [
    "# Visualizador de grupo de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524f1752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.343876Z",
     "start_time": "2022-08-24T03:57:45.331882Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 128, 128)):\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=4)\n",
    "    # image_grid.permute.squeeze se encargará de convertir el tensor de 3 canales(RGB)\n",
    "    # en una sola imagen de 1 canal\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437babcb",
   "metadata": {},
   "source": [
    "# Puntos de guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2167bca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.358878Z",
     "start_time": "2022-08-24T03:57:45.346879Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(states, is_best, output_dir,\n",
    "                    filename='checkpoint.pth'):\n",
    "    torch.save(states, os.path.join(output_dir, filename))\n",
    "    if is_best:\n",
    "        torch.save(states, os.path.join(output_dir, 'checkpoint_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456608c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04a61067",
   "metadata": {},
   "source": [
    "# Ratio de aprendizaje variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0430774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.374880Z",
     "start_time": "2022-08-24T03:57:45.365883Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearLrDecay(object):\n",
    "    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n",
    "\n",
    "        assert start_lr > end_lr\n",
    "        self.optimizer = optimizer\n",
    "        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n",
    "        self.decay_start_step = decay_start_step\n",
    "        self.decay_end_step = decay_end_step\n",
    "        self.start_lr = start_lr\n",
    "        self.end_lr = end_lr\n",
    "\n",
    "    def step(self, current_step):\n",
    "        if current_step <= self.decay_start_step:\n",
    "            lr = self.start_lr\n",
    "        elif current_step >= self.decay_end_step:\n",
    "            lr = self.end_lr\n",
    "        else:\n",
    "            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c0f2c",
   "metadata": {},
   "source": [
    "# Inicialización de pesos, ruido y generación de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3272df50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.390876Z",
     "start_time": "2022-08-24T03:57:45.377878Z"
    }
   },
   "outputs": [],
   "source": [
    "def inits_weight(m):\n",
    "        if type(m) == nn.Linear:\n",
    "                nn.init.xavier_uniform_(m.weight.data, 1.)\n",
    "\n",
    "\n",
    "def noise(imgs, latent_dim):\n",
    "        return torch.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "def gener_noise(gener_batch_size, latent_dim):\n",
    "        return torch.FloatTensor(np.random.normal(0, 1, (gener_batch_size, latent_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bb4f9",
   "metadata": {},
   "source": [
    "# Capa MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861e32d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.406882Z",
     "start_time": "2022-08-24T03:57:45.393878Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feat, hid_feat=None, out_feat=None,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        if not hid_feat:\n",
    "            hid_feat = in_feat\n",
    "        if not out_feat:\n",
    "            out_feat = in_feat\n",
    "        self.fc1 = nn.Linear(in_feat, hid_feat)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hid_feat, out_feat)\n",
    "        self.droprateout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.droprateout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb7cc1",
   "metadata": {},
   "source": [
    "# Bloque de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beea4bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.422878Z",
     "start_time": "2022-08-24T03:57:45.409877Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, attention_dropout=0.2, proj_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = 1./dim**0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(proj_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.heads, c//self.heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        dot = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = dot.softmax(dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909c3ba",
   "metadata": {},
   "source": [
    "# Tratamiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23942c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.438897Z",
     "start_time": "2022-08-24T03:57:45.425898Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImgPatches(nn.Module):\n",
    "    def __init__(self, input_channel=3, dim=768, patch_size=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(input_channel, dim,\n",
    "                                     kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patch_embed(img).flatten(2).transpose(1, 2)\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d2b8e",
   "metadata": {},
   "source": [
    "# Proceso de UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb4386c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.454900Z",
     "start_time": "2022-08-24T03:57:45.441898Z"
    }
   },
   "outputs": [],
   "source": [
    "def UpSampling(x, H, W):\n",
    "        B, N, C = x.size()\n",
    "        assert N == H*W\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.view(-1, C, H, W)\n",
    "        x = nn.PixelShuffle(2)(x)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.view(-1, C, H*W)\n",
    "        x = x.permute(0,2,1)\n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730790ca",
   "metadata": {},
   "source": [
    "# Bloque de Codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d12e9cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.470903Z",
     "start_time": "2022-08-24T03:57:45.457902Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, drop_rate, drop_rate)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, dim*mlp_ratio, dropout=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x = x + self.attn(x1)\n",
    "        x2 = self.ln2(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726357b0",
   "metadata": {},
   "source": [
    "# Sección Codificadora del Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e31524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.486900Z",
     "start_time": "2022-08-24T03:57:45.474900Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, depth, dim, heads, mlp_ratio=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.Encoder_Blocks = nn.ModuleList([\n",
    "            Encoder_Block(dim, heads, mlp_ratio, drop_rate)\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for Encoder_Block in self.Encoder_Blocks:\n",
    "            x = Encoder_Block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbe2e8",
   "metadata": {},
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "\n",
    " https://arxiv.org/pdf/2006.10738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f65208a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.518895Z",
     "start_time": "2022-08-24T03:57:45.489899Z"
    }
   },
   "outputs": [],
   "source": [
    "def DiffAugment(x, policy='', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.2):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    if random.random() < 0.3:\n",
    "        cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "        )\n",
    "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "        mask[grid_batch, grid_x, grid_y] = 0\n",
    "        x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "def rand_rotate(x, ratio=0.5):\n",
    "    k = random.randint(1,3)\n",
    "    if random.random() < ratio:\n",
    "        x = torch.rot90(x, k, [2,3])\n",
    "    return x\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "    'rotate': [rand_rotate],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa69e4a",
   "metadata": {},
   "source": [
    "# Modelo del generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18081918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.566899Z",
     "start_time": "2022-08-24T03:57:45.537899Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, depth1=5, depth2=4, depth3=2, initial_size=8, dim=384, heads=4, mlp_ratio=4, drop_rate=0., patch_size = 4, input_channel=3, diff_aug  = \"translation,cutout,color\"):\n",
    "#     def __init__(self, depth1=4, depth2=3, depth3=2, initial_size=8, dim=384, heads=4, mlp_ratio=4, drop_rate=0.):\n",
    "        super(Generator_2, self).__init__()\n",
    "\n",
    "        self.initial_size = initial_size\n",
    "        self.dim = dim\n",
    "        self.depth1 = depth1\n",
    "        self.depth2 = depth2\n",
    "        self.depth3 = depth3\n",
    "        self.heads = heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        ##\n",
    "        self.diff_aug = diff_aug\n",
    "        self.patch_size = patch_size\n",
    "        ##\n",
    "        self.droprate_rate = drop_rate\n",
    "\n",
    "#         1024 == 32 x 32 (tamaño deseado de las imágenes)\n",
    "#         self.mlp = nn.Linear(1024, (self.initial_size ** 2) * self.dim)\n",
    "#         self.mlp = nn.Linear(3072, (self.initial_size ** 2) * self.dim)\n",
    "        # PLUS╦\n",
    "        self.class_embedding = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        # Image patches and embedding layer\n",
    "        self.patches = ImgPatches(input_channel, dim, self.patch_size)\n",
    "#         self.positional_embedding = nn.Parameter(torch.zeros(1, num_patches+1, dim))\n",
    "        # PLUS╩\n",
    "        self.positional_embedding_1 = nn.Parameter(torch.zeros(1, (8**2), dim))\n",
    "        self.positional_embedding_2 = nn.Parameter(\n",
    "            torch.zeros(1, (8*2)**2, dim//4))\n",
    "        self.positional_embedding_3 = nn.Parameter(\n",
    "            torch.zeros(1, (8*4)**2, dim//16))\n",
    "\n",
    "        self.TransformerEncoder_encoder1 = TransformerEncoder(depth=self.depth1,\n",
    "                                                              dim=self.dim,\n",
    "                                                              heads=self.heads,\n",
    "                                                              mlp_ratio=self.mlp_ratio,\n",
    "                                                              drop_rate=self.droprate_rate)\n",
    "\n",
    "        self.TransformerEncoder_encoder2 = TransformerEncoder(depth=self.depth2,\n",
    "                                                              dim=self.dim//4,\n",
    "                                                              heads=self.heads,\n",
    "                                                              mlp_ratio=self.mlp_ratio,\n",
    "                                                              drop_rate=self.droprate_rate)\n",
    "\n",
    "        self.TransformerEncoder_encoder3 = TransformerEncoder(depth=self.depth3,\n",
    "                                                              dim=self.dim//16,\n",
    "                                                              heads=self.heads,\n",
    "                                                              mlp_ratio=self.mlp_ratio,\n",
    "                                                              drop_rate=self.droprate_rate)\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Conv2d(self.dim//16, 3, 1, 1, 0))\n",
    "\n",
    "    def forward(self, noise):\n",
    "#         x = self.mlp(noise)\n",
    "#         x = x.view(-1, self.initial_size ** 2, self.dim)\n",
    "#         x = x + self.positional_embedding_1\n",
    "\n",
    "    # PLUS╦\n",
    "        x = DiffAugment(noise, self.diff_aug)\n",
    "        x = self.patches(x)\n",
    "        x += self.positional_embedding_1\n",
    "    # PLUS╩    \n",
    "#         x = x + self.positional_embedding_1\n",
    "        H, W = self.initial_size, self.initial_size\n",
    "        x = self.TransformerEncoder_encoder1(x)\n",
    "        x, H, W = UpSampling(x, H, W)\n",
    "        x = x + self.positional_embedding_2\n",
    "        x = self.TransformerEncoder_encoder2(x)\n",
    "        x, H, W = UpSampling(x, H, W)\n",
    "        x = x + self.positional_embedding_3\n",
    "        x = self.TransformerEncoder_encoder3(x)\n",
    "        x = self.linear(x.permute(0, 2, 1).view(-1, self.dim//16, H, W))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b5e59",
   "metadata": {},
   "source": [
    "# Modelo Discriminador (basado en Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b260b003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.598898Z",
     "start_time": "2022-08-24T03:57:45.569897Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, diff_aug, image_size=32, patch_size=4,\n",
    "                 input_channel=3, num_classes=1,\n",
    "                 dim=384, depth=7, heads=4, mlp_ratio=4,\n",
    "                 drop_rate=0.):\n",
    "        super().__init__()\n",
    "        if image_size % patch_size != 0:\n",
    "            raise ValueError('Error en la dimensión de la imagen.')\n",
    "        num_patches = (image_size//patch_size) ** 2\n",
    "        self.diff_aug = diff_aug\n",
    "        self.patch_size = patch_size\n",
    "        self.depth = depth\n",
    "        # Image patches and embedding layer\n",
    "        self.patches = ImgPatches(input_channel, dim, self.patch_size)\n",
    "\n",
    "        # Embedding for patch position and class\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1,\n",
    "                                                             num_patches+1,\n",
    "                                                             dim))\n",
    "        self.class_embedding = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        nn.init.trunc_normal_(self.positional_embedding, std=0.2)\n",
    "        nn.init.trunc_normal_(self.class_embedding, std=0.2)\n",
    "\n",
    "        self.droprate = nn.Dropout(p=drop_rate)\n",
    "        self.TransfomerEncoder = TransformerEncoder(depth, dim, heads,\n",
    "                                                    mlp_ratio, drop_rate)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.out = nn.Linear(dim, num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = DiffAugment(x, self.diff_aug)\n",
    "        b = x.shape[0]\n",
    "        cls_token = self.class_embedding.expand(b, -1, -1)\n",
    "        x = self.patches(x)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x += self.positional_embedding\n",
    "        x = self.droprate(x)\n",
    "        x = self.TransfomerEncoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2b899",
   "metadata": {},
   "source": [
    "# Modelo Discriminador (basado en Convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612a686",
   "metadata": {},
   "source": [
    "# Seteo de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb6156c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.614899Z",
     "start_time": "2022-08-24T03:57:45.601897Z"
    }
   },
   "outputs": [],
   "source": [
    "image_size = 32 # Size of image for discriminator input\n",
    "initial_size = 8 #Initial size for generator.\n",
    "patch_size = 4  # Patch size for generated image.\n",
    "num_classes = 1  # Number of classes for discriminator.\n",
    "lr_gen = 0.0001  # Learning rate for generator.\n",
    "lr_dis = 0.0001  # Learning rate for discriminator.\n",
    "weight_decay = 1e-3  # Weight decay.\n",
    "latent_dim = 1024  # Latent dimension.\n",
    "n_critic = 5  # n_critic.\n",
    "max_iter = 400000  # max_iter.\n",
    "gener_batch_size = 32  # Batch size for generator.\n",
    "dis_batch_size = 32  # Batch size for discriminator.\n",
    "epoch = 50  # Number of epoch.\n",
    "output_dir = 'checkpoint'  # Checkpoint.\n",
    "dim = 384  # Embedding dimension.\n",
    "#dim = 256\n",
    "img_name = \"img_name\"  # Name of pictures file.\n",
    "# loss = \"available\"  # Loss function\n",
    "loss = None\n",
    "phi = 1  # phi\n",
    "beta1 = 0 #beta1\n",
    "beta2 = 0.99 # beta2\n",
    "lr_decay = True # lr_decay\n",
    "diff_aug = \"translation,cutout,color\" # help='Data Augmentation\n",
    "best = 1e4  # Best lr for Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99f421",
   "metadata": {},
   "source": [
    "# Visualizador de datos por paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed0de56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.645898Z",
     "start_time": "2022-08-24T03:57:45.618898Z"
    }
   },
   "outputs": [],
   "source": [
    "writer=SummaryWriter()\n",
    "writer_dict = {'writer':writer}\n",
    "writer_dict[\"train_global_steps\"]=0\n",
    "writer_dict[\"valid_global_steps\"]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2839be9",
   "metadata": {},
   "source": [
    "# Cálculo de penalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca384cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.661896Z",
     "start_time": "2022-08-24T03:57:45.648898Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples, phi):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(real_samples.get_device())\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.get_device())\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.contiguous().view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05916f2",
   "metadata": {},
   "source": [
    "# Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "493d9c0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.709899Z",
     "start_time": "2022-08-24T03:57:45.680899Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_v2(gen_dataloader, disc_dataloader,\n",
    "             generator, discriminator, optim_gen, optim_dis,\n",
    "             epoch, writer, schedulers, img_size=32, latent_dim=latent_dim,\n",
    "             n_critic=n_critic, gener_batch_size=gener_batch_size,\n",
    "             device=\"cuda:0\"):\n",
    "\n",
    "    writer = writer_dict['writer']\n",
    "    gen_step = 0\n",
    "\n",
    "    generator = generator.train()\n",
    "    discriminator = discriminator.train()\n",
    "\n",
    "    gen_iterator = iter(gen_dataloader)\n",
    "\n",
    "    for index, (img, _) in enumerate(disc_dataloader):\n",
    "\n",
    "        try:\n",
    "            (data2, _) = next(gen_iterator)\n",
    "        except StopIteration:\n",
    "            \n",
    "            gen_iterator = iter(gen_dataloader)\n",
    "            (data2, _) = next(gen_iterator)\n",
    "\n",
    "        global_steps = writer_dict['train_global_steps']\n",
    "\n",
    "        real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "#         fake_imgs = data2.view(16,3072).type(torch.cuda.FloatTensor)        \n",
    "        fake_imgs = data2.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        ######\n",
    "#         input(\"optim_dis.zero_grad() - Press enter to continue\")\n",
    "        #####\n",
    "        optim_dis.zero_grad()\n",
    "\n",
    "        real_valid = discriminator(real_imgs)\n",
    "\n",
    "        # obtenemos un batch de imágenes fake generadas por el\n",
    "        # modelo generador sin entrenamiento\n",
    "        fake_imgs = generator(fake_imgs).detach()\n",
    "\n",
    "#         print(fake_imgs.detach().shape)\n",
    "        # Hallamos la predicción del discriminador\n",
    "        fake_valid = discriminator(fake_imgs)\n",
    "\n",
    "        if loss is not None:\n",
    "            loss_dis = torch.mean(nn.ReLU(inplace=True)(1.0 - real_valid)).to(\n",
    "                device) + torch.mean(nn.ReLU(inplace=True)(1 + fake_valid)).to(device)\n",
    "        else:\n",
    "            gradient_penalty = compute_gradient_penalty(\n",
    "                discriminator, real_imgs, fake_imgs.detach(), phi)\n",
    "            loss_dis = -torch.mean(real_valid) + torch.mean(fake_valid) + \\\n",
    "                gradient_penalty * 10 / (phi ** 2)\n",
    "\n",
    "        loss_dis.backward()\n",
    "        optim_dis.step()\n",
    "\n",
    "        writer.add_scalar(\"loss_dis\", loss_dis.item(), global_steps)\n",
    "\n",
    "        if global_steps % n_critic == 0:\n",
    "\n",
    "            optim_gen.zero_grad()\n",
    "            if schedulers:\n",
    "                gen_scheduler, dis_scheduler = schedulers\n",
    "                g_lr = gen_scheduler.step(global_steps)\n",
    "                d_lr = dis_scheduler.step(global_steps)\n",
    "                writer.add_scalar('LR/g_lr', g_lr, global_steps)\n",
    "                writer.add_scalar('LR/d_lr', d_lr, global_steps)\n",
    "\n",
    "            generated_imgs = generator(data2.type(torch.cuda.FloatTensor))\n",
    "            \n",
    "            # Obtenemos la puntuación del discriminador para la retroalimentación\n",
    "            fake_valid = discriminator(generated_imgs)\n",
    "\n",
    "            gener_loss = -torch.mean(fake_valid).to(device)\n",
    "            gener_loss.backward()\n",
    "            optim_gen.step()\n",
    "            writer.add_scalar(\"gener_loss\", gener_loss.item(), global_steps)\n",
    "\n",
    "            gen_step += 1\n",
    "\n",
    "        if gen_step and index % 250 == 0:\n",
    "            sample_imgs = generated_imgs[:16]\n",
    "            img_grid = make_grid(sample_imgs, nrow=4,\n",
    "                                 normalize=True, scale_each=True)\n",
    "            save_image(\n",
    "                sample_imgs, f'./generated_imgs_3/generated_img_{epoch}_{index % len(disc_dataloader)}.png', nrow=4, normalize=True, scale_each=True)\n",
    "            tqdm.write(\"[Epoch %d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" %\n",
    "                       (epoch+1, index % len(disc_dataloader), len(disc_dataloader), loss_dis.item(), gener_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dded5bb",
   "metadata": {},
   "source": [
    "# Obtención de puntuación FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0650c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72763d51",
   "metadata": {},
   "source": [
    "# Función de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "337c84bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.725730Z",
     "start_time": "2022-08-24T03:57:45.712899Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(generator, writer_dict, fid_stat):\n",
    "\n",
    "    writer = writer_dict['writer']\n",
    "    global_steps = writer_dict['valid_global_steps']\n",
    "\n",
    "    generator = generator.eval()\n",
    "    fid_score = get_fid(fid_stat, epoch, generator, num_img=5000,\n",
    "                        val_batch_size=60*2, latent_dim=1024,\n",
    "                        writer_dict=None, cls_idx=None)\n",
    "\n",
    "    print(f\"FID score: {fid_score}\")\n",
    "\n",
    "    writer.add_scalar('FID_score', fid_score, global_steps)\n",
    "\n",
    "    writer_dict['valid_global_steps'] = global_steps + 1\n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ff898",
   "metadata": {},
   "source": [
    "# Pasos previos al entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e661593b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:45.757728Z",
     "start_time": "2022-08-24T03:57:45.740731Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generator= Generator(depth1=5, depth2=4, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\n",
    "# # generator= Generator(depth1=4, depth2=3, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\n",
    "# generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd18713e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:46.185729Z",
     "start_time": "2022-08-24T03:57:45.760729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator_2(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (TransformerEncoder_encoder1): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (TransformerEncoder_encoder2): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (TransformerEncoder_encoder3): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=24, out_features=72, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=24, out_features=24, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=24, out_features=96, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=96, out_features=24, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=24, out_features=72, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=24, out_features=24, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=24, out_features=96, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=96, out_features=24, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator= Generator_2(depth1=5, depth2=4, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.2)\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c48f44e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:46.547031Z",
     "start_time": "2022-08-24T03:57:46.188730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (droprate): Dropout(p=0.2, inplace=False)\n",
       "  (TransfomerEncoder): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=384, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(diff_aug = \"translation,cutout,color\", image_size=32, patch_size=4, input_channel=3, num_classes=1,\n",
    "                 dim=dim, depth=7, heads=4, mlp_ratio=4, drop_rate=0.2)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e55a698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:49.314948Z",
     "start_time": "2022-08-24T03:57:46.550029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (droprate): Dropout(p=0.2, inplace=False)\n",
       "  (TransfomerEncoder): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=384, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.apply(inits_weight)\n",
    "discriminator.apply(inits_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1960509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:49.331055Z",
     "start_time": "2022-08-24T03:57:49.317941Z"
    }
   },
   "outputs": [],
   "source": [
    "optim_gen = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "                              generator.parameters()), \n",
    "                       lr=lr_gen, \n",
    "                       betas=(beta1, beta2))\n",
    "\n",
    "optim_dis = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "                                  discriminator.parameters()),\n",
    "                           lr=lr_dis, \n",
    "                           betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c8bd612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T03:57:49.347049Z",
     "start_time": "2022-08-24T03:57:49.335051Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_scheduler = LinearLrDecay(optim_gen, \n",
    "                              lr_gen, \n",
    "                              0.0, 0, \n",
    "                              max_iter * n_critic)\n",
    "\n",
    "dis_scheduler = LinearLrDecay(optim_dis, \n",
    "                              lr_dis, \n",
    "                              0.0, 0, \n",
    "                              max_iter * n_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad3bfb",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19019884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T13:36:03.915021Z",
     "start_time": "2022-08-24T03:57:49.350051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] [Batch 0/3737] [D loss: 4.558729] [G loss: -1.708445]\n",
      "[Epoch 1] [Batch 250/3737] [D loss: -2.903322] [G loss: 2.415651]\n",
      "[Epoch 1] [Batch 500/3737] [D loss: 1.359362] [G loss: 6.722465]\n",
      "[Epoch 1] [Batch 750/3737] [D loss: 0.078255] [G loss: 4.246788]\n",
      "[Epoch 1] [Batch 1000/3737] [D loss: 0.217818] [G loss: 5.992769]\n",
      "[Epoch 1] [Batch 1250/3737] [D loss: -0.409512] [G loss: 4.614217]\n",
      "[Epoch 1] [Batch 1500/3737] [D loss: 0.290705] [G loss: 3.215325]\n",
      "[Epoch 1] [Batch 1750/3737] [D loss: 0.307381] [G loss: 3.627505]\n",
      "[Epoch 1] [Batch 2000/3737] [D loss: -0.613294] [G loss: 3.512439]\n",
      "[Epoch 1] [Batch 2250/3737] [D loss: 0.063526] [G loss: 2.947756]\n",
      "[Epoch 1] [Batch 2500/3737] [D loss: -0.326890] [G loss: 3.893158]\n",
      "[Epoch 1] [Batch 2750/3737] [D loss: 0.327816] [G loss: 4.037427]\n",
      "[Epoch 1] [Batch 3000/3737] [D loss: 0.476959] [G loss: 2.684521]\n",
      "[Epoch 1] [Batch 3250/3737] [D loss: 0.483207] [G loss: 2.270751]\n",
      "[Epoch 1] [Batch 3500/3737] [D loss: 0.159442] [G loss: 2.632469]\n",
      "[Epoch 2] [Batch 0/3737] [D loss: 0.819750] [G loss: 3.284298]\n",
      "[Epoch 2] [Batch 250/3737] [D loss: 0.358078] [G loss: 4.027675]\n",
      "[Epoch 2] [Batch 500/3737] [D loss: 0.063297] [G loss: 3.608192]\n",
      "[Epoch 2] [Batch 750/3737] [D loss: -0.031239] [G loss: 3.491786]\n",
      "[Epoch 2] [Batch 1000/3737] [D loss: 0.737980] [G loss: 3.628214]\n",
      "[Epoch 2] [Batch 1250/3737] [D loss: 0.158630] [G loss: 3.827044]\n",
      "[Epoch 2] [Batch 1500/3737] [D loss: -0.440038] [G loss: 3.327244]\n",
      "[Epoch 2] [Batch 1750/3737] [D loss: 0.064549] [G loss: 1.962086]\n",
      "[Epoch 2] [Batch 2000/3737] [D loss: -0.559791] [G loss: 1.900447]\n",
      "[Epoch 2] [Batch 2250/3737] [D loss: 0.010806] [G loss: 1.469292]\n",
      "[Epoch 2] [Batch 2500/3737] [D loss: 0.553033] [G loss: 1.534199]\n",
      "[Epoch 2] [Batch 2750/3737] [D loss: 0.328224] [G loss: 1.653407]\n",
      "[Epoch 2] [Batch 3000/3737] [D loss: 0.335135] [G loss: 1.567207]\n",
      "[Epoch 2] [Batch 3250/3737] [D loss: 0.073399] [G loss: 1.191135]\n",
      "[Epoch 2] [Batch 3500/3737] [D loss: -0.256918] [G loss: 1.711726]\n",
      "[Epoch 3] [Batch 0/3737] [D loss: -0.301583] [G loss: 2.313235]\n",
      "[Epoch 3] [Batch 250/3737] [D loss: 0.707160] [G loss: 2.578809]\n",
      "[Epoch 3] [Batch 500/3737] [D loss: 0.014039] [G loss: 2.655477]\n",
      "[Epoch 3] [Batch 750/3737] [D loss: -0.015242] [G loss: 3.234713]\n",
      "[Epoch 3] [Batch 1000/3737] [D loss: -0.061902] [G loss: 3.511540]\n",
      "[Epoch 3] [Batch 1250/3737] [D loss: 0.106409] [G loss: 3.426846]\n",
      "[Epoch 3] [Batch 1500/3737] [D loss: 0.145840] [G loss: 3.436006]\n",
      "[Epoch 3] [Batch 1750/3737] [D loss: 0.308381] [G loss: 3.449441]\n",
      "[Epoch 3] [Batch 2000/3737] [D loss: 0.413336] [G loss: 3.587543]\n",
      "[Epoch 3] [Batch 2250/3737] [D loss: 0.275623] [G loss: 2.618576]\n",
      "[Epoch 3] [Batch 2500/3737] [D loss: 0.142313] [G loss: 3.099748]\n",
      "[Epoch 3] [Batch 2750/3737] [D loss: -0.650408] [G loss: 2.785806]\n",
      "[Epoch 3] [Batch 3000/3737] [D loss: 0.044040] [G loss: 2.742953]\n",
      "[Epoch 3] [Batch 3250/3737] [D loss: 0.130905] [G loss: 1.637320]\n",
      "[Epoch 3] [Batch 3500/3737] [D loss: 0.208516] [G loss: 2.644356]\n",
      "[Epoch 4] [Batch 0/3737] [D loss: -0.366712] [G loss: 2.419560]\n",
      "[Epoch 4] [Batch 250/3737] [D loss: -0.272044] [G loss: 1.644462]\n",
      "[Epoch 4] [Batch 500/3737] [D loss: -0.248819] [G loss: 1.238309]\n",
      "[Epoch 4] [Batch 750/3737] [D loss: -0.476623] [G loss: 1.395164]\n",
      "[Epoch 4] [Batch 1000/3737] [D loss: -0.027161] [G loss: 0.375959]\n",
      "[Epoch 4] [Batch 1250/3737] [D loss: 0.348888] [G loss: 1.055698]\n",
      "[Epoch 4] [Batch 1500/3737] [D loss: 0.075015] [G loss: 0.891167]\n",
      "[Epoch 4] [Batch 1750/3737] [D loss: 0.070752] [G loss: 1.926852]\n",
      "[Epoch 4] [Batch 2000/3737] [D loss: 0.229984] [G loss: 1.597361]\n",
      "[Epoch 4] [Batch 2250/3737] [D loss: 0.607476] [G loss: 1.945177]\n",
      "[Epoch 4] [Batch 2500/3737] [D loss: 0.098482] [G loss: 1.699317]\n",
      "[Epoch 4] [Batch 2750/3737] [D loss: -0.056667] [G loss: 1.705792]\n",
      "[Epoch 4] [Batch 3000/3737] [D loss: 0.620976] [G loss: 1.698449]\n",
      "[Epoch 4] [Batch 3250/3737] [D loss: 0.474818] [G loss: 1.752401]\n",
      "[Epoch 4] [Batch 3500/3737] [D loss: -0.087593] [G loss: 1.822838]\n",
      "[Epoch 5] [Batch 0/3737] [D loss: -0.064682] [G loss: 1.661486]\n",
      "[Epoch 5] [Batch 250/3737] [D loss: 0.151950] [G loss: 1.116349]\n",
      "[Epoch 5] [Batch 500/3737] [D loss: 0.099862] [G loss: 1.860557]\n",
      "[Epoch 5] [Batch 750/3737] [D loss: -0.209391] [G loss: 1.678115]\n",
      "[Epoch 5] [Batch 1000/3737] [D loss: 0.122053] [G loss: 2.089897]\n",
      "[Epoch 5] [Batch 1250/3737] [D loss: -0.075778] [G loss: 2.046661]\n",
      "[Epoch 5] [Batch 1500/3737] [D loss: 0.153413] [G loss: 2.136325]\n",
      "[Epoch 5] [Batch 1750/3737] [D loss: 0.137848] [G loss: 2.797165]\n",
      "[Epoch 5] [Batch 2000/3737] [D loss: 0.039727] [G loss: 2.596517]\n",
      "[Epoch 5] [Batch 2250/3737] [D loss: 0.221373] [G loss: 1.786201]\n",
      "[Epoch 5] [Batch 2500/3737] [D loss: -0.022741] [G loss: 3.025531]\n",
      "[Epoch 5] [Batch 2750/3737] [D loss: 0.581010] [G loss: 2.968033]\n",
      "[Epoch 5] [Batch 3000/3737] [D loss: 0.703589] [G loss: 2.533151]\n",
      "[Epoch 5] [Batch 3250/3737] [D loss: 0.060758] [G loss: 2.036901]\n",
      "[Epoch 5] [Batch 3500/3737] [D loss: 0.548221] [G loss: 2.115932]\n",
      "[Epoch 6] [Batch 0/3737] [D loss: -0.166903] [G loss: 1.717765]\n",
      "[Epoch 6] [Batch 250/3737] [D loss: 0.189423] [G loss: 2.336774]\n",
      "[Epoch 6] [Batch 500/3737] [D loss: 0.717554] [G loss: 2.194299]\n",
      "[Epoch 6] [Batch 750/3737] [D loss: -0.013693] [G loss: 1.821105]\n",
      "[Epoch 6] [Batch 1000/3737] [D loss: 0.001959] [G loss: 1.720373]\n",
      "[Epoch 6] [Batch 1250/3737] [D loss: -0.130535] [G loss: 1.715988]\n",
      "[Epoch 6] [Batch 1500/3737] [D loss: 0.508523] [G loss: 1.042994]\n",
      "[Epoch 6] [Batch 1750/3737] [D loss: 0.009875] [G loss: 2.788997]\n",
      "[Epoch 6] [Batch 2000/3737] [D loss: 0.273763] [G loss: 2.518637]\n",
      "[Epoch 6] [Batch 2250/3737] [D loss: -0.078043] [G loss: 2.171801]\n",
      "[Epoch 6] [Batch 2500/3737] [D loss: 0.658705] [G loss: 2.160336]\n",
      "[Epoch 6] [Batch 2750/3737] [D loss: 0.531001] [G loss: 2.080021]\n",
      "[Epoch 6] [Batch 3000/3737] [D loss: 0.149254] [G loss: 2.102910]\n",
      "[Epoch 6] [Batch 3250/3737] [D loss: 0.326885] [G loss: 2.811460]\n",
      "[Epoch 6] [Batch 3500/3737] [D loss: 0.721010] [G loss: 2.228934]\n",
      "[Epoch 7] [Batch 0/3737] [D loss: 0.032447] [G loss: 2.015826]\n",
      "[Epoch 7] [Batch 250/3737] [D loss: 0.288022] [G loss: 1.636510]\n",
      "[Epoch 7] [Batch 500/3737] [D loss: 0.610331] [G loss: 1.742406]\n",
      "[Epoch 7] [Batch 750/3737] [D loss: 0.003098] [G loss: 2.342873]\n",
      "[Epoch 7] [Batch 1000/3737] [D loss: 0.103958] [G loss: 2.682781]\n",
      "[Epoch 7] [Batch 1250/3737] [D loss: -0.146247] [G loss: 1.930715]\n",
      "[Epoch 7] [Batch 1500/3737] [D loss: 0.241501] [G loss: 2.153752]\n",
      "[Epoch 7] [Batch 1750/3737] [D loss: -0.012798] [G loss: 2.211967]\n",
      "[Epoch 7] [Batch 2000/3737] [D loss: 0.041353] [G loss: 1.727383]\n",
      "[Epoch 7] [Batch 2250/3737] [D loss: -0.031657] [G loss: 1.243485]\n",
      "[Epoch 7] [Batch 2500/3737] [D loss: 0.464509] [G loss: 1.336044]\n",
      "[Epoch 7] [Batch 2750/3737] [D loss: -0.330975] [G loss: 0.953304]\n",
      "[Epoch 7] [Batch 3000/3737] [D loss: 0.432325] [G loss: 1.553096]\n",
      "[Epoch 7] [Batch 3250/3737] [D loss: 0.455625] [G loss: 1.622712]\n",
      "[Epoch 7] [Batch 3500/3737] [D loss: 0.393528] [G loss: 1.599961]\n",
      "[Epoch 8] [Batch 0/3737] [D loss: 0.446911] [G loss: 1.011925]\n",
      "[Epoch 8] [Batch 250/3737] [D loss: 0.758039] [G loss: 1.239294]\n",
      "[Epoch 8] [Batch 500/3737] [D loss: 0.142934] [G loss: 1.360491]\n",
      "[Epoch 8] [Batch 750/3737] [D loss: -0.513592] [G loss: 1.920403]\n",
      "[Epoch 8] [Batch 1000/3737] [D loss: 0.420418] [G loss: 1.408564]\n",
      "[Epoch 8] [Batch 1250/3737] [D loss: -0.222205] [G loss: 2.033470]\n",
      "[Epoch 8] [Batch 1500/3737] [D loss: -0.004575] [G loss: 1.839602]\n",
      "[Epoch 8] [Batch 1750/3737] [D loss: 0.461659] [G loss: 1.372778]\n",
      "[Epoch 8] [Batch 2000/3737] [D loss: -0.073291] [G loss: 2.116078]\n",
      "[Epoch 8] [Batch 2250/3737] [D loss: 0.171976] [G loss: 2.139258]\n",
      "[Epoch 8] [Batch 2500/3737] [D loss: 0.114414] [G loss: 2.029828]\n",
      "[Epoch 8] [Batch 2750/3737] [D loss: -0.222406] [G loss: 1.763504]\n",
      "[Epoch 8] [Batch 3000/3737] [D loss: 0.019158] [G loss: 1.769135]\n",
      "[Epoch 8] [Batch 3250/3737] [D loss: 0.126099] [G loss: 1.877102]\n",
      "[Epoch 8] [Batch 3500/3737] [D loss: 0.238665] [G loss: 1.266600]\n",
      "[Epoch 9] [Batch 0/3737] [D loss: 0.416096] [G loss: 1.936190]\n",
      "[Epoch 9] [Batch 250/3737] [D loss: 0.017582] [G loss: 1.432119]\n",
      "[Epoch 9] [Batch 500/3737] [D loss: -0.058909] [G loss: 1.850579]\n",
      "[Epoch 9] [Batch 750/3737] [D loss: -0.312578] [G loss: 1.828989]\n",
      "[Epoch 9] [Batch 1000/3737] [D loss: -0.092183] [G loss: 1.140152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] [Batch 1250/3737] [D loss: 0.273452] [G loss: 1.738003]\n",
      "[Epoch 9] [Batch 1500/3737] [D loss: 0.143791] [G loss: 1.796231]\n",
      "[Epoch 9] [Batch 1750/3737] [D loss: 0.115110] [G loss: 1.165984]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     lr_schedulers \u001b[38;5;241m=\u001b[39m (gen_scheduler, dis_scheduler) \u001b[38;5;28;01mif\u001b[39;00m lr_decay \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     train(noise, generator, discriminator, optim_gen, optim_dis,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#           epoch, writer, lr_schedulers, img_size=32, latent_dim=latent_dim,\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#           n_critic=n_critic, gener_batch_size=gener_batch_size)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtrain_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_loader_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_loader_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_dis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedulers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mn_critic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgener_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgener_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_fid\u001b[39m\u001b[38;5;124m'\u001b[39m: best}\n\u001b[0;32m     14\u001b[0m     checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtrain_v2\u001b[1;34m(gen_dataloader, disc_dataloader, generator, discriminator, optim_gen, optim_dis, epoch, writer, schedulers, img_size, latent_dim, n_critic, gener_batch_size, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m generated_imgs \u001b[38;5;241m=\u001b[39m generator(data2\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mFloatTensor))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Obtenemos la puntuación del discriminador para la retroalimentación\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m fake_valid \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m gener_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(fake_valid)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m gener_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdroprate(x)\n\u001b[1;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransfomerEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x[:, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m Encoder_Block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoder_Blocks:\n\u001b[1;32m---> 10\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder_Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mEncoder_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x2)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     b, n, c \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 16\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(b, n, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, c\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads)\n\u001b[0;32m     17\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     19\u001b[0m     dot \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):\n",
    "\n",
    "    lr_schedulers = (gen_scheduler, dis_scheduler) if lr_decay else None\n",
    "\n",
    "#     train(noise, generator, discriminator, optim_gen, optim_dis,\n",
    "#           epoch, writer, lr_schedulers, img_size=32, latent_dim=latent_dim,\n",
    "#           n_critic=n_critic, gener_batch_size=gener_batch_size)\n",
    "    \n",
    "    train_v2(dt_loader_gen, dt_loader_real, generator, discriminator, optim_gen, optim_dis,\n",
    "          epoch, writer, lr_schedulers, img_size=32, latent_dim=latent_dim,\n",
    "          n_critic=n_critic, gener_batch_size=gener_batch_size)\n",
    "\n",
    "    checkpoint = {'epoch': epoch, 'best_fid': best}\n",
    "    checkpoint['generator_state_dict'] = generator.state_dict()\n",
    "    checkpoint['discriminator_state_dict'] = discriminator.state_dict()\n",
    "    save_checkpoint(checkpoint, is_best=True, output_dir=output_dir)\n",
    "#     score = validate(generator, writer_dict, fid_stat)\n",
    "\n",
    "#     print(f'FID score: {score} - best ID score: {best} || @ epoch {epoch+1}.')\n",
    "#     if epoch == 0 or epoch > 30:\n",
    "#         if score < best:\n",
    "#             \n",
    "#             best = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0fab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3668a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4863a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753c9ffa",
   "metadata": {},
   "source": [
    "# TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d645ab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T04:06:35.748455Z",
     "start_time": "2022-08-08T04:06:35.748455Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_pixeled = './Test_images/pixeled/'\n",
    "\n",
    "# Tamaño que deseamos que tengan las imágenes\n",
    "image_size = 32\n",
    "# Tamaño del lote de imágenes\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "pixeled_imgs_test = dtst.ImageFolder(\n",
    "    root=data_folder_pixeled,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # Esta desviación estandar y media es la hallada para los datos de\n",
    "        # entrenamiento\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "\n",
    "\n",
    "dt_loader_gen_test = DataLoader(pixeled_imgs_test,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=1,\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294d6f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T04:06:35.750458Z",
     "start_time": "2022-08-08T04:06:35.750458Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (img, _) in enumerate(dt_loader_gen_test):   \n",
    "       \n",
    "    fake_imgs = img.view(16,3072).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    generated_imgs = generator(fake_imgs)\n",
    "    \n",
    "    show_tensor_images(generated_imgs,16,size=(3,32,32))\n",
    "    sample_imgs = generated_imgs[:16]\n",
    "#     img_grid = make_grid(sample_imgs, nrow=4,\n",
    "#     normalize=True, scale_each=True)\n",
    "    save_image(sample_imgs, f'./generated_imgs_test/test_img_{index % len(dt_loader_gen_test)}.jpg', nrow=4, normalize=True, scale_each=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ab91d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T04:06:35.752455Z",
     "start_time": "2022-08-08T04:06:35.752455Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_complete = './Test_images/original/'\n",
    "\n",
    "complete_imgs_test = dtst.ImageFolder(\n",
    "    root=data_folder_complete,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # El problema encontrado es que necesitamos hallar la desviación estandar\n",
    "        # media de toda la información para realizar una correcta normalización\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "dt_loader_real_test = DataLoader(complete_imgs_test,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=1,\n",
    "                       drop_last=True)\n",
    "\n",
    "for index, (img, _) in enumerate(dt_loader_real_test):   \n",
    "       \n",
    "    real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    show_tensor_images(real_imgs,16,size=(3,32,32))\n",
    "    sample_imgs = real_imgs[:16]\n",
    "\n",
    "    save_image(sample_imgs, f'./generated_imgs_test/real2/test_img_{index % len(dt_loader_real_test)}.png', nrow=4, normalize=True, scale_each=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d23ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "324px",
    "width": "357px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.844px",
    "left": "596px",
    "right": "20px",
    "top": "136px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
