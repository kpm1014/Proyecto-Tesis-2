{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473d4af4",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0dbcfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:35.591827Z",
     "start_time": "2022-08-14T04:43:35.573829Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798fa2bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:39.084831Z",
     "start_time": "2022-08-14T04:43:35.595831Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.datasets as dtst\n",
    "\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Retiramos los mensajes de alerta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d49cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:40.463831Z",
     "start_time": "2022-08-14T04:43:39.089847Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba7ef2",
   "metadata": {},
   "source": [
    "# Configuraciones para la tarjeta de video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53d15f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:40.495833Z",
     "start_time": "2022-08-14T04:43:40.474841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# El sistema selecciona el hardware disponible\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dae917d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:40.527827Z",
     "start_time": "2022-08-14T04:43:40.503838Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Ti', major=8, minor=6, total_memory=8191MB, multi_processor_count=38)\n"
     ]
    }
   ],
   "source": [
    "# Verificamos qué Hardware estamos usando\n",
    "if device == \"cuda:0\":\n",
    "    print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df127870",
   "metadata": {},
   "source": [
    "# Definimos el DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070df2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:41.927826Z",
     "start_time": "2022-08-14T04:43:40.531833Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_complete = './Images/original/'\n",
    "data_folder_pixeled = './Images/pixeled/'\n",
    "# Tamaño que deseamos que tengan las imágenes\n",
    "image_size = 32\n",
    "# Tamaño del lote de imágenes\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "dsimgs_1 = dtst.ImageFolder(\n",
    "    root=data_folder_complete,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # El problema encontrado es que necesitamos hallar la desviación estandar\n",
    "        # media de toda la información para realizar una correcta normalización\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "dsimgs_2 = dtst.ImageFolder(\n",
    "    root=data_folder_pixeled,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # Esta desviación estandar y media es la hallada para los datos de\n",
    "        # entrenamiento\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91cb37b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:41.943827Z",
     "start_time": "2022-08-14T04:43:41.931831Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_loader_real = DataLoader(dsimgs_1,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=2,\n",
    "                       drop_last=True)\n",
    "\n",
    "dt_loader_gen = DataLoader(dsimgs_2,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=2,\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea53080",
   "metadata": {},
   "source": [
    "# Visualizador de grupo de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524f1752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:41.958828Z",
     "start_time": "2022-08-14T04:43:41.947831Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 128, 128)):\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=4)\n",
    "    # image_grid.permute.squeeze se encargará de convertir el tensor de 3 canales(RGB)\n",
    "    # en una sola imagen de 1 canal\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437babcb",
   "metadata": {},
   "source": [
    "# Puntos de guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2167bca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:41.974829Z",
     "start_time": "2022-08-14T04:43:41.962831Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(states, is_best, output_dir,\n",
    "                    filename='checkpoint.pth'):\n",
    "    torch.save(states, os.path.join(output_dir, filename))\n",
    "    if is_best:\n",
    "        torch.save(states, os.path.join(output_dir, 'checkpoint_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456608c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04a61067",
   "metadata": {},
   "source": [
    "# Ratio de aprendizaje variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0430774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.006831Z",
     "start_time": "2022-08-14T04:43:41.982832Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearLrDecay(object):\n",
    "    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n",
    "\n",
    "        assert start_lr > end_lr\n",
    "        self.optimizer = optimizer\n",
    "        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n",
    "        self.decay_start_step = decay_start_step\n",
    "        self.decay_end_step = decay_end_step\n",
    "        self.start_lr = start_lr\n",
    "        self.end_lr = end_lr\n",
    "\n",
    "    def step(self, current_step):\n",
    "        if current_step <= self.decay_start_step:\n",
    "            lr = self.start_lr\n",
    "        elif current_step >= self.decay_end_step:\n",
    "            lr = self.end_lr\n",
    "        else:\n",
    "            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c0f2c",
   "metadata": {},
   "source": [
    "# Inicialización de pesos, ruido y generación de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3272df50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.022826Z",
     "start_time": "2022-08-14T04:43:42.010829Z"
    }
   },
   "outputs": [],
   "source": [
    "def inits_weight(m):\n",
    "        if type(m) == nn.Linear:\n",
    "                nn.init.xavier_uniform_(m.weight.data, 1.)\n",
    "\n",
    "\n",
    "def noise(imgs, latent_dim):\n",
    "        return torch.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "def gener_noise(gener_batch_size, latent_dim):\n",
    "        return torch.FloatTensor(np.random.normal(0, 1, (gener_batch_size, latent_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bb4f9",
   "metadata": {},
   "source": [
    "# Capa MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861e32d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.038828Z",
     "start_time": "2022-08-14T04:43:42.026829Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feat, hid_feat=None, out_feat=None,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        if not hid_feat:\n",
    "            hid_feat = in_feat\n",
    "        if not out_feat:\n",
    "            out_feat = in_feat\n",
    "        self.fc1 = nn.Linear(in_feat, hid_feat)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hid_feat, out_feat)\n",
    "        self.droprateout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.droprateout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb7cc1",
   "metadata": {},
   "source": [
    "# Bloque de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beea4bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.054827Z",
     "start_time": "2022-08-14T04:43:42.041829Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, attention_dropout=0., proj_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = 1./dim**0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(proj_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.heads, c//self.heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        dot = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = dot.softmax(dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909c3ba",
   "metadata": {},
   "source": [
    "# Tratamiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23942c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.070827Z",
     "start_time": "2022-08-14T04:43:42.057829Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImgPatches(nn.Module):\n",
    "    def __init__(self, input_channel=3, dim=768, patch_size=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(input_channel, dim,\n",
    "                                     kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patch_embed(img).flatten(2).transpose(1, 2)\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d2b8e",
   "metadata": {},
   "source": [
    "# Proceso de UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb4386c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.086826Z",
     "start_time": "2022-08-14T04:43:42.074829Z"
    }
   },
   "outputs": [],
   "source": [
    "def UpSampling(x, H, W):\n",
    "        B, N, C = x.size()\n",
    "        assert N == H*W\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.view(-1, C, H, W)\n",
    "        x = nn.PixelShuffle(2)(x)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.view(-1, C, H*W)\n",
    "        x = x.permute(0,2,1)\n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730790ca",
   "metadata": {},
   "source": [
    "# Bloque de Codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d12e9cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.102829Z",
     "start_time": "2022-08-14T04:43:42.089829Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, drop_rate, drop_rate)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, dim*mlp_ratio, dropout=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x = x + self.attn(x1)\n",
    "        x2 = self.ln2(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726357b0",
   "metadata": {},
   "source": [
    "# Sección Codificadora del Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e31524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.118827Z",
     "start_time": "2022-08-14T04:43:42.105831Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, depth, dim, heads, mlp_ratio=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.Encoder_Blocks = nn.ModuleList([\n",
    "            Encoder_Block(dim, heads, mlp_ratio, drop_rate)\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for Encoder_Block in self.Encoder_Blocks:\n",
    "            x = Encoder_Block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbe2e8",
   "metadata": {},
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "\n",
    " https://arxiv.org/pdf/2006.10738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f65208a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.166827Z",
     "start_time": "2022-08-14T04:43:42.121835Z"
    }
   },
   "outputs": [],
   "source": [
    "def DiffAugment(x, policy='', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.2):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    if random.random() < 0.3:\n",
    "        cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "        )\n",
    "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "        mask[grid_batch, grid_x, grid_y] = 0\n",
    "        x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "def rand_rotate(x, ratio=0.5):\n",
    "    k = random.randint(1,3)\n",
    "    if random.random() < ratio:\n",
    "        x = torch.rot90(x, k, [2,3])\n",
    "    return x\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "    'rotate': [rand_rotate],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa69e4a",
   "metadata": {},
   "source": [
    "# Modelo del generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31065899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.181828Z",
     "start_time": "2022-08-14T04:43:42.169835Z"
    }
   },
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "    \n",
    "#     def __init__(self, depth1=5, depth2=4, depth3=2, initial_size=8, dim=384, heads=4, mlp_ratio=4, drop_rate=0.):\n",
    "# #     def __init__(self, depth1=4, depth2=3, depth3=2, initial_size=8, dim=384, heads=4, mlp_ratio=4, drop_rate=0.):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         self.initial_size = initial_size\n",
    "#         self.dim = dim\n",
    "#         self.depth1 = depth1\n",
    "#         self.depth2 = depth2\n",
    "#         self.depth3 = depth3\n",
    "#         self.heads = heads\n",
    "#         self.mlp_ratio = mlp_ratio\n",
    "#         self.droprate_rate = drop_rate\n",
    "\n",
    "# #         1024 == 32 x 32 (tamaño deseado de las imágenes)\n",
    "# #         self.mlp = nn.Linear(1024, (self.initial_size ** 2) * self.dim)\n",
    "#         self.mlp = nn.Linear(3072, (self.initial_size ** 2) * self.dim)\n",
    "\n",
    "#         self.positional_embedding_1 = nn.Parameter(torch.zeros(1, (8**2), dim))\n",
    "#         self.positional_embedding_2 = nn.Parameter(\n",
    "#             torch.zeros(1, (8*2)**2, dim//4))\n",
    "#         self.positional_embedding_3 = nn.Parameter(\n",
    "#             torch.zeros(1, (8*4)**2, dim//16))\n",
    "\n",
    "#         self.TransformerEncoder_encoder1 = TransformerEncoder(depth=self.depth1,\n",
    "#                                                               dim=self.dim,\n",
    "#                                                               heads=self.heads,\n",
    "#                                                               mlp_ratio=self.mlp_ratio,\n",
    "#                                                               drop_rate=self.droprate_rate)\n",
    "\n",
    "#         self.TransformerEncoder_encoder2 = TransformerEncoder(depth=self.depth2,\n",
    "#                                                               dim=self.dim//4,\n",
    "#                                                               heads=self.heads,\n",
    "#                                                               mlp_ratio=self.mlp_ratio,\n",
    "#                                                               drop_rate=self.droprate_rate)\n",
    "\n",
    "#         self.TransformerEncoder_encoder3 = TransformerEncoder(depth=self.depth3,\n",
    "#                                                               dim=self.dim//16,\n",
    "#                                                               heads=self.heads,\n",
    "#                                                               mlp_ratio=self.mlp_ratio,\n",
    "#                                                               drop_rate=self.droprate_rate)\n",
    "\n",
    "#         self.linear = nn.Sequential(nn.Conv2d(self.dim//16, 3, 1, 1, 0))\n",
    "\n",
    "#     def forward(self, noise):\n",
    "# #         print('##### GENERADOR #####\\n')\n",
    "# #         print('>> noise.size():', noise.size())\n",
    "\n",
    "#         x = self.mlp(noise)\n",
    "# #         print('>> x after MLP size:', x.size())\n",
    "#         x = x.view(-1, self.initial_size ** 2, self.dim)\n",
    "\n",
    "# #         print('>> x after view size :', x.size())\n",
    "\n",
    "#         x = x + self.positional_embedding_1\n",
    "# #         print('>> x+embedding:', x.size())\n",
    "#         H, W = self.initial_size, self.initial_size\n",
    "# #         print('H, W:', H, W)\n",
    "#         x = self.TransformerEncoder_encoder1(x)\n",
    "# #         print('>> X after first encoder:', x.size())\n",
    "#         x, H, W = UpSampling(x, H, W)\n",
    "\n",
    "# #         print('>> X after UPSAMPLING encoder:', x.size())\n",
    "#         x = x + self.positional_embedding_2\n",
    "\n",
    "#         x = self.TransformerEncoder_encoder2(x)\n",
    "\n",
    "# #         print('>> X after second encoder:', x.size())\n",
    "\n",
    "#         x, H, W = UpSampling(x, H, W)\n",
    "# #         print('>> X after 2ND UPSAMPLING encoder:', x.size())\n",
    "#         x = x + self.positional_embedding_3\n",
    "\n",
    "#         x = self.TransformerEncoder_encoder3(x)\n",
    "# #         print('>> X after third encoder:', x.size())\n",
    "#         x = self.linear(x.permute(0, 2, 1).view(-1, self.dim//16, H, W))\n",
    "\n",
    "# #         print('>> X after linear layer:', x.size())\n",
    "# #         print('\\n#################\\n')\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18081918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.213827Z",
     "start_time": "2022-08-14T04:43:42.184830Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, depth1=5, depth2=4, depth3=2, initial_size=8, dim=384, heads=4, mlp_ratio=4, drop_rate=0., patch_size = 4, input_channel=3, diff_aug  = \"translation,cutout,color\"):\n",
    "#     def __init__(self, depth1=4, depth2=3, depth3=2, initial_size=8, dim=384, heads=4, mlp_ratio=4, drop_rate=0.):\n",
    "        super(Generator_2, self).__init__()\n",
    "\n",
    "        self.initial_size = initial_size\n",
    "        self.dim = dim\n",
    "        self.depth1 = depth1\n",
    "        self.depth2 = depth2\n",
    "        self.depth3 = depth3\n",
    "        self.heads = heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        ##\n",
    "        self.diff_aug = diff_aug\n",
    "        self.patch_size = patch_size\n",
    "        ##\n",
    "        self.droprate_rate = drop_rate\n",
    "\n",
    "#         1024 == 32 x 32 (tamaño deseado de las imágenes)\n",
    "#         self.mlp = nn.Linear(1024, (self.initial_size ** 2) * self.dim)\n",
    "#         self.mlp = nn.Linear(3072, (self.initial_size ** 2) * self.dim)\n",
    "        # PLUS╦\n",
    "        self.class_embedding = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        # Image patches and embedding layer\n",
    "        self.patches = ImgPatches(input_channel, dim, self.patch_size)\n",
    "#         self.positional_embedding = nn.Parameter(torch.zeros(1, num_patches+1, dim))\n",
    "        # PLUS╩\n",
    "        self.positional_embedding_1 = nn.Parameter(torch.zeros(1, (8**2), dim))\n",
    "        self.positional_embedding_2 = nn.Parameter(\n",
    "            torch.zeros(1, (8*2)**2, dim//4))\n",
    "        self.positional_embedding_3 = nn.Parameter(\n",
    "            torch.zeros(1, (8*4)**2, dim//16))\n",
    "\n",
    "        self.TransformerEncoder_encoder1 = TransformerEncoder(depth=self.depth1,\n",
    "                                                              dim=self.dim,\n",
    "                                                              heads=self.heads,\n",
    "                                                              mlp_ratio=self.mlp_ratio,\n",
    "                                                              drop_rate=self.droprate_rate)\n",
    "\n",
    "        self.TransformerEncoder_encoder2 = TransformerEncoder(depth=self.depth2,\n",
    "                                                              dim=self.dim//4,\n",
    "                                                              heads=self.heads,\n",
    "                                                              mlp_ratio=self.mlp_ratio,\n",
    "                                                              drop_rate=self.droprate_rate)\n",
    "\n",
    "        self.TransformerEncoder_encoder3 = TransformerEncoder(depth=self.depth3,\n",
    "                                                              dim=self.dim//16,\n",
    "                                                              heads=self.heads,\n",
    "                                                              mlp_ratio=self.mlp_ratio,\n",
    "                                                              drop_rate=self.droprate_rate)\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Conv2d(self.dim//16, 3, 1, 1, 0))\n",
    "\n",
    "    def forward(self, noise):\n",
    "#         x = self.mlp(noise)\n",
    "#         x = x.view(-1, self.initial_size ** 2, self.dim)\n",
    "#         x = x + self.positional_embedding_1\n",
    "\n",
    "    # PLUS╦\n",
    "        x = DiffAugment(noise, self.diff_aug)\n",
    "        x = self.patches(x)\n",
    "        x += self.positional_embedding_1\n",
    "    # PLUS╩    \n",
    "#         x = x + self.positional_embedding_1\n",
    "        H, W = self.initial_size, self.initial_size\n",
    "        x = self.TransformerEncoder_encoder1(x)\n",
    "        x, H, W = UpSampling(x, H, W)\n",
    "        x = x + self.positional_embedding_2\n",
    "        x = self.TransformerEncoder_encoder2(x)\n",
    "        x, H, W = UpSampling(x, H, W)\n",
    "        x = x + self.positional_embedding_3\n",
    "        x = self.TransformerEncoder_encoder3(x)\n",
    "        x = self.linear(x.permute(0, 2, 1).view(-1, self.dim//16, H, W))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b5e59",
   "metadata": {},
   "source": [
    "# Modelo Discriminador (basado en Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b260b003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.244826Z",
     "start_time": "2022-08-14T04:43:42.217830Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, diff_aug, image_size=32, patch_size=4,\n",
    "                 input_channel=3, num_classes=1,\n",
    "                 dim=384, depth=7, heads=4, mlp_ratio=4,\n",
    "                 drop_rate=0.):\n",
    "        super().__init__()\n",
    "        if image_size % patch_size != 0:\n",
    "            raise ValueError('Error en la dimensión de la imagen.')\n",
    "        num_patches = (image_size//patch_size) ** 2\n",
    "        self.diff_aug = diff_aug\n",
    "        self.patch_size = patch_size\n",
    "        self.depth = depth\n",
    "        # Image patches and embedding layer\n",
    "        self.patches = ImgPatches(input_channel, dim, self.patch_size)\n",
    "\n",
    "        # Embedding for patch position and class\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1,\n",
    "                                                             num_patches+1,\n",
    "                                                             dim))\n",
    "        self.class_embedding = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        nn.init.trunc_normal_(self.positional_embedding, std=0.2)\n",
    "        nn.init.trunc_normal_(self.class_embedding, std=0.2)\n",
    "\n",
    "        self.droprate = nn.Dropout(p=drop_rate)\n",
    "        self.TransfomerEncoder = TransformerEncoder(depth, dim, heads,\n",
    "                                                    mlp_ratio, drop_rate)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.out = nn.Linear(dim, num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = DiffAugment(x, self.diff_aug)\n",
    "        b = x.shape[0]\n",
    "        cls_token = self.class_embedding.expand(b, -1, -1)\n",
    "        x = self.patches(x)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x += self.positional_embedding\n",
    "        x = self.droprate(x)\n",
    "        x = self.TransfomerEncoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2b899",
   "metadata": {},
   "source": [
    "# Modelo Discriminador (basado en Convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612a686",
   "metadata": {},
   "source": [
    "# Seteo de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb6156c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.260829Z",
     "start_time": "2022-08-14T04:43:42.247829Z"
    }
   },
   "outputs": [],
   "source": [
    "image_size = 32 # Size of image for discriminator input\n",
    "initial_size = 8 #Initial size for generator.\n",
    "patch_size = 4  # Patch size for generated image.\n",
    "num_classes = 1  # Number of classes for discriminator.\n",
    "lr_gen = 0.0001  # Learning rate for generator.\n",
    "lr_dis = 0.0001  # Learning rate for discriminator.\n",
    "weight_decay = 1e-3  # Weight decay.\n",
    "latent_dim = 1024  # Latent dimension.\n",
    "n_critic = 5  # n_critic.\n",
    "max_iter = 400000  # max_iter.\n",
    "gener_batch_size = 32  # Batch size for generator.\n",
    "dis_batch_size = 32  # Batch size for discriminator.\n",
    "epoch = 50  # Number of epoch.\n",
    "output_dir = 'checkpoint'  # Checkpoint.\n",
    "dim = 384  # Embedding dimension.\n",
    "#dim = 256\n",
    "img_name = \"img_name\"  # Name of pictures file.\n",
    "# loss = \"available\"  # Loss function\n",
    "loss = None\n",
    "phi = 1  # phi\n",
    "beta1 = 0 #beta1\n",
    "beta2 = 0.99 # beta2\n",
    "lr_decay = True # lr_decay\n",
    "diff_aug = \"translation,cutout,color\" # help='Data Augmentation\n",
    "best = 1e4  # Best lr for Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99f421",
   "metadata": {},
   "source": [
    "# Visualizador de datos por paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed0de56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.276834Z",
     "start_time": "2022-08-14T04:43:42.264831Z"
    }
   },
   "outputs": [],
   "source": [
    "writer=SummaryWriter()\n",
    "writer_dict = {'writer':writer}\n",
    "writer_dict[\"train_global_steps\"]=0\n",
    "writer_dict[\"valid_global_steps\"]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2839be9",
   "metadata": {},
   "source": [
    "# Cálculo de penalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca384cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.292828Z",
     "start_time": "2022-08-14T04:43:42.280829Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples, phi):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(real_samples.get_device())\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.get_device())\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.contiguous().view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05916f2",
   "metadata": {},
   "source": [
    "# Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2376a00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.308827Z",
     "start_time": "2022-08-14T04:43:42.296828Z"
    }
   },
   "outputs": [],
   "source": [
    "# def train(noise, generator, discriminator, optim_gen, optim_dis,\n",
    "#           epoch, writer, schedulers, img_size=32, latent_dim=latent_dim,\n",
    "#           n_critic=n_critic,\n",
    "#           gener_batch_size=gener_batch_size, device=\"cuda:0\"):\n",
    "\n",
    "#     writer = writer_dict['writer']\n",
    "#     gen_step = 0\n",
    "\n",
    "#     generator = generator.train()\n",
    "#     discriminator = discriminator.train()\n",
    "\n",
    "#     ######\n",
    "# #     input(\"Retransformando las imágenes - Press enter to continue\")\n",
    "#     #####\n",
    "#     transform = transforms.Compose([transforms.Resize(size=(img_size, img_size)), transforms.RandomHorizontalFlip(\n",
    "#     ), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#     ######\n",
    "# #     input(\"Inicializas una nueva variablea para dt_loader_real - Press enter to continue\")\n",
    "#     #####\n",
    "#     train_loader = dt_loader_real\n",
    "\n",
    "#     for index, (img, _) in enumerate(train_loader):\n",
    "\n",
    "#         global_steps = writer_dict['train_global_steps']\n",
    "\n",
    "#         ######\n",
    "# #         input(\"convertimos imágenes a tensores float - Press enter to continue\")\n",
    "#         #####\n",
    "#         real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "\n",
    "#         ######\n",
    "# #         input(\"Ruido 1 - Press enter to continue\")\n",
    "#         #####\n",
    "#         noise = torch.cuda.FloatTensor(\n",
    "#             np.random.normal(0, 1, (img.shape[0], latent_dim)))\n",
    "#         ######\n",
    "# #         input(\"optim_dis.zero_grad() - Press enter to continue\")\n",
    "#         #####\n",
    "#         optim_dis.zero_grad()\n",
    "#         ######\n",
    "# #         input(\"discriminator(real_imgs) - Press enter to continue + 1.0GB\")\n",
    "#         #####\n",
    "#         real_valid = discriminator(real_imgs)\n",
    "#         ######\n",
    "# #         input(\"generator(noise).detach() - Press enter to continue + 1.9GB\")\n",
    "#         #####\n",
    "#         fake_imgs = generator(noise).detach()\n",
    "#         ######\n",
    "# #         input(\"discriminator(fake_imgs) - Press enter to continue + 0.1GB\")\n",
    "#         #####\n",
    "#         fake_valid = discriminator(fake_imgs)\n",
    "\n",
    "#         if loss == 'hinge':\n",
    "#             loss_dis = torch.mean(nn.ReLU(inplace=True)(1.0 - real_valid)).to(\n",
    "#                 device) + torch.mean(nn.ReLU(inplace=True)(1 + fake_valid)).to(device)\n",
    "#         elif loss == 'wgangp_eps':\n",
    "#             gradient_penalty = compute_gradient_penalty(\n",
    "#                 discriminator, real_imgs, fake_imgs.detach(), phi)\n",
    "#             loss_dis = -torch.mean(real_valid) + torch.mean(fake_valid) + \\\n",
    "#                 gradient_penalty * 10 / (phi ** 2)\n",
    "\n",
    "#         loss_dis.backward()\n",
    "#         optim_dis.step()\n",
    "\n",
    "#         writer.add_scalar(\"loss_dis\", loss_dis.item(), global_steps)\n",
    "\n",
    "#         if global_steps % n_critic == 0:\n",
    "\n",
    "#             optim_gen.zero_grad()\n",
    "#             if schedulers:\n",
    "#                 gen_scheduler, dis_scheduler = schedulers\n",
    "#                 g_lr = gen_scheduler.step(global_steps)\n",
    "#                 d_lr = dis_scheduler.step(global_steps)\n",
    "#                 writer.add_scalar('LR/g_lr', g_lr, global_steps)\n",
    "#                 writer.add_scalar('LR/d_lr', d_lr, global_steps)\n",
    "\n",
    "#             ######\n",
    "# #             input(\"Generating noise tensors - Press enter to continue + 0.1GB\")\n",
    "#             #####\n",
    "#             gener_noise = torch.cuda.FloatTensor(\n",
    "#                 np.random.normal(0, 1, (gener_batch_size, latent_dim)))\n",
    "\n",
    "#             ######\n",
    "# #             input(\"generator(gener_noise) - Press enter to continue + 0.0GB\")\n",
    "#             #####\n",
    "#             generated_imgs = generator(gener_noise)\n",
    "#             ######\n",
    "# #             input(\"Press enter to continue\")\n",
    "#             #####\n",
    "#             fake_valid = discriminator(generated_imgs)\n",
    "\n",
    "#             gener_loss = -torch.mean(fake_valid).to(device)\n",
    "#             gener_loss.backward()\n",
    "#             optim_gen.step()\n",
    "#             writer.add_scalar(\"gener_loss\", gener_loss.item(), global_steps)\n",
    "\n",
    "#             gen_step += 1\n",
    "\n",
    "#         if gen_step and index % 100 == 0:\n",
    "#             sample_imgs = generated_imgs[:8]\n",
    "#             img_grid = make_grid(sample_imgs, nrow=4,\n",
    "#                                  normalize=True, scale_each=True)\n",
    "#             save_image(\n",
    "#                 sample_imgs, f'./generated_imgs/generated_img_{epoch}_{index % len(train_loader)}.jpg', nrow=4, normalize=True, scale_each=True)\n",
    "#             tqdm.write(\"[Epoch %d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" %\n",
    "#                        (epoch+1, index % len(train_loader), len(train_loader), loss_dis.item(), gener_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "493d9c0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.340826Z",
     "start_time": "2022-08-14T04:43:42.312831Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_v2(gen_dataloader, disc_dataloader,\n",
    "             generator, discriminator, optim_gen, optim_dis,\n",
    "             epoch, writer, schedulers, img_size=32, latent_dim=latent_dim,\n",
    "             n_critic=n_critic, gener_batch_size=gener_batch_size,\n",
    "             device=\"cuda:0\"):\n",
    "\n",
    "    writer = writer_dict['writer']\n",
    "    gen_step = 0\n",
    "\n",
    "    generator = generator.train()\n",
    "    discriminator = discriminator.train()\n",
    "\n",
    "    gen_iterator = iter(gen_dataloader)\n",
    "\n",
    "    for index, (img, _) in enumerate(disc_dataloader):\n",
    "\n",
    "        try:\n",
    "            (data2, _) = next(gen_iterator)\n",
    "        except StopIteration:\n",
    "            \n",
    "            gen_iterator = iter(gen_dataloader)\n",
    "            (data2, _) = next(gen_iterator)\n",
    "\n",
    "        global_steps = writer_dict['train_global_steps']\n",
    "\n",
    "        real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "#         fake_imgs = data2.view(16,3072).type(torch.cuda.FloatTensor)        \n",
    "        fake_imgs = data2.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        ######\n",
    "#         input(\"optim_dis.zero_grad() - Press enter to continue\")\n",
    "        #####\n",
    "        optim_dis.zero_grad()\n",
    "\n",
    "        real_valid = discriminator(real_imgs)\n",
    "\n",
    "        # obtenemos un batch de imágenes fake generadas por el\n",
    "        # modelo generador sin entrenamiento\n",
    "        fake_imgs = generator(fake_imgs).detach()\n",
    "\n",
    "#         print(fake_imgs.detach().shape)\n",
    "        # Hallamos la predicción del discriminador\n",
    "        fake_valid = discriminator(fake_imgs)\n",
    "\n",
    "        if loss is not None:\n",
    "            loss_dis = torch.mean(nn.ReLU(inplace=True)(1.0 - real_valid)).to(\n",
    "                device) + torch.mean(nn.ReLU(inplace=True)(1 + fake_valid)).to(device)\n",
    "        else:\n",
    "            gradient_penalty = compute_gradient_penalty(\n",
    "                discriminator, real_imgs, fake_imgs.detach(), phi)\n",
    "            loss_dis = -torch.mean(real_valid) + torch.mean(fake_valid) + \\\n",
    "                gradient_penalty * 10 / (phi ** 2)\n",
    "\n",
    "        loss_dis.backward()\n",
    "        optim_dis.step()\n",
    "\n",
    "        writer.add_scalar(\"loss_dis\", loss_dis.item(), global_steps)\n",
    "\n",
    "        if global_steps % n_critic == 0:\n",
    "\n",
    "            optim_gen.zero_grad()\n",
    "            if schedulers:\n",
    "                gen_scheduler, dis_scheduler = schedulers\n",
    "                g_lr = gen_scheduler.step(global_steps)\n",
    "                d_lr = dis_scheduler.step(global_steps)\n",
    "                writer.add_scalar('LR/g_lr', g_lr, global_steps)\n",
    "                writer.add_scalar('LR/d_lr', d_lr, global_steps)\n",
    "\n",
    "            generated_imgs = generator(data2.type(torch.cuda.FloatTensor))\n",
    "            \n",
    "            # Obtenemos la puntuación del discriminador para la retroalimentación\n",
    "            fake_valid = discriminator(generated_imgs)\n",
    "\n",
    "            gener_loss = -torch.mean(fake_valid).to(device)\n",
    "            gener_loss.backward()\n",
    "            optim_gen.step()\n",
    "            writer.add_scalar(\"gener_loss\", gener_loss.item(), global_steps)\n",
    "\n",
    "            gen_step += 1\n",
    "\n",
    "        if gen_step and index % 250 == 0:\n",
    "            sample_imgs = generated_imgs[:16]\n",
    "            img_grid = make_grid(sample_imgs, nrow=4,\n",
    "                                 normalize=True, scale_each=True)\n",
    "            save_image(\n",
    "                sample_imgs, f'./generated_imgs_2/generated_img_{epoch}_{index % len(disc_dataloader)}.png', nrow=4, normalize=True, scale_each=True)\n",
    "            tqdm.write(\"[Epoch %d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" %\n",
    "                       (epoch+1, index % len(disc_dataloader), len(disc_dataloader), loss_dis.item(), gener_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dded5bb",
   "metadata": {},
   "source": [
    "# Obtención de puntuación FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0650c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72763d51",
   "metadata": {},
   "source": [
    "# Función de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "337c84bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.356826Z",
     "start_time": "2022-08-14T04:43:42.343830Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(generator, writer_dict, fid_stat):\n",
    "\n",
    "    writer = writer_dict['writer']\n",
    "    global_steps = writer_dict['valid_global_steps']\n",
    "\n",
    "    generator = generator.eval()\n",
    "    fid_score = get_fid(fid_stat, epoch, generator, num_img=5000,\n",
    "                        val_batch_size=60*2, latent_dim=1024,\n",
    "                        writer_dict=None, cls_idx=None)\n",
    "\n",
    "    print(f\"FID score: {fid_score}\")\n",
    "\n",
    "    writer.add_scalar('FID_score', fid_score, global_steps)\n",
    "\n",
    "    writer_dict['valid_global_steps'] = global_steps + 1\n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ff898",
   "metadata": {},
   "source": [
    "# Pasos previos al entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e661593b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.388831Z",
     "start_time": "2022-08-14T04:43:42.372833Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generator= Generator(depth1=5, depth2=4, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\n",
    "# # generator= Generator(depth1=4, depth2=3, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\n",
    "# generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd18713e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:42.724826Z",
     "start_time": "2022-08-14T04:43:42.392834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator_2(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (TransformerEncoder_encoder1): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (TransformerEncoder_encoder2): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (TransformerEncoder_encoder3): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=24, out_features=72, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=24, out_features=24, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=24, out_features=96, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=96, out_features=24, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=24, out_features=72, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=24, out_features=24, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=24, out_features=96, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=96, out_features=24, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator= Generator_2(depth1=5, depth2=4, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\n",
    "# generator= Generator(depth1=4, depth2=3, depth3=2, initial_size=8, dim=dim, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c48f44e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:43.107826Z",
     "start_time": "2022-08-14T04:43:42.727831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (droprate): Dropout(p=0.0, inplace=False)\n",
       "  (TransfomerEncoder): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=384, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(diff_aug = \"translation,cutout,color\", image_size=32, patch_size=4, input_channel=3, num_classes=1,\n",
    "                 dim=dim, depth=7, heads=4, mlp_ratio=4, drop_rate=0.)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e55a698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:44.435831Z",
     "start_time": "2022-08-14T04:43:43.110831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (droprate): Dropout(p=0.0, inplace=False)\n",
       "  (TransfomerEncoder): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Encoder_Block(\n",
       "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (droprateout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=384, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.apply(inits_weight)\n",
    "discriminator.apply(inits_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1960509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:44.451826Z",
     "start_time": "2022-08-14T04:43:44.438828Z"
    }
   },
   "outputs": [],
   "source": [
    "optim_gen = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "                              generator.parameters()), \n",
    "                       lr=lr_gen, \n",
    "                       betas=(beta1, beta2))\n",
    "\n",
    "optim_dis = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "                                  discriminator.parameters()),\n",
    "                           lr=lr_dis, \n",
    "                           betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c8bd612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-14T04:43:44.466825Z",
     "start_time": "2022-08-14T04:43:44.454829Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_scheduler = LinearLrDecay(optim_gen, \n",
    "                              lr_gen, \n",
    "                              0.0, 0, \n",
    "                              max_iter * n_critic)\n",
    "\n",
    "dis_scheduler = LinearLrDecay(optim_dis, \n",
    "                              lr_dis, \n",
    "                              0.0, 0, \n",
    "                              max_iter * n_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad3bfb",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19019884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T00:22:05.761171Z",
     "start_time": "2022-08-14T04:43:44.469847Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] [Batch 0/3737] [D loss: 4.928785] [G loss: 0.423911]\n",
      "[Epoch 1] [Batch 250/3737] [D loss: -5.603291] [G loss: 0.846895]\n",
      "[Epoch 1] [Batch 500/3737] [D loss: -3.148864] [G loss: -1.865978]\n",
      "[Epoch 1] [Batch 750/3737] [D loss: -0.248189] [G loss: 2.153396]\n",
      "[Epoch 1] [Batch 1000/3737] [D loss: -0.616114] [G loss: 0.722374]\n",
      "[Epoch 1] [Batch 1250/3737] [D loss: -0.083865] [G loss: 0.016641]\n",
      "[Epoch 1] [Batch 1500/3737] [D loss: -0.743816] [G loss: 0.572345]\n",
      "[Epoch 1] [Batch 1750/3737] [D loss: -0.555105] [G loss: 1.173501]\n",
      "[Epoch 1] [Batch 2000/3737] [D loss: -0.411012] [G loss: 0.874375]\n",
      "[Epoch 1] [Batch 2250/3737] [D loss: -0.270281] [G loss: 1.187196]\n",
      "[Epoch 1] [Batch 2500/3737] [D loss: -0.493823] [G loss: 1.818932]\n",
      "[Epoch 1] [Batch 2750/3737] [D loss: -0.219764] [G loss: -0.033861]\n",
      "[Epoch 1] [Batch 3000/3737] [D loss: -0.407539] [G loss: 1.039855]\n",
      "[Epoch 1] [Batch 3250/3737] [D loss: -0.039022] [G loss: -0.026963]\n",
      "[Epoch 1] [Batch 3500/3737] [D loss: -0.413641] [G loss: 0.596636]\n",
      "[Epoch 2] [Batch 0/3737] [D loss: 0.177219] [G loss: 0.764259]\n",
      "[Epoch 2] [Batch 250/3737] [D loss: -0.468426] [G loss: 0.894062]\n",
      "[Epoch 2] [Batch 500/3737] [D loss: -0.322493] [G loss: 1.618803]\n",
      "[Epoch 2] [Batch 750/3737] [D loss: -0.494531] [G loss: 0.519771]\n",
      "[Epoch 2] [Batch 1000/3737] [D loss: 0.155702] [G loss: 0.961878]\n",
      "[Epoch 2] [Batch 1250/3737] [D loss: -0.029180] [G loss: 0.939241]\n",
      "[Epoch 2] [Batch 1500/3737] [D loss: -0.804934] [G loss: 0.972816]\n",
      "[Epoch 2] [Batch 1750/3737] [D loss: -0.516512] [G loss: 0.615599]\n",
      "[Epoch 2] [Batch 2000/3737] [D loss: -0.365934] [G loss: 0.686240]\n",
      "[Epoch 2] [Batch 2250/3737] [D loss: -0.095776] [G loss: 0.801472]\n",
      "[Epoch 2] [Batch 2500/3737] [D loss: -0.421976] [G loss: 0.945708]\n",
      "[Epoch 2] [Batch 2750/3737] [D loss: -0.100352] [G loss: 1.756483]\n",
      "[Epoch 2] [Batch 3000/3737] [D loss: 0.067875] [G loss: 1.331556]\n",
      "[Epoch 2] [Batch 3250/3737] [D loss: -0.717885] [G loss: 0.923100]\n",
      "[Epoch 2] [Batch 3500/3737] [D loss: -0.121550] [G loss: 1.551632]\n",
      "[Epoch 3] [Batch 0/3737] [D loss: -0.290262] [G loss: 0.642155]\n",
      "[Epoch 3] [Batch 250/3737] [D loss: -0.161498] [G loss: 1.120264]\n",
      "[Epoch 3] [Batch 500/3737] [D loss: -0.326634] [G loss: 2.499482]\n",
      "[Epoch 3] [Batch 750/3737] [D loss: 0.011125] [G loss: 1.182044]\n",
      "[Epoch 3] [Batch 1000/3737] [D loss: -0.865735] [G loss: 1.411005]\n",
      "[Epoch 3] [Batch 1250/3737] [D loss: -0.212308] [G loss: 2.186936]\n",
      "[Epoch 3] [Batch 1500/3737] [D loss: -0.600487] [G loss: 1.398752]\n",
      "[Epoch 3] [Batch 1750/3737] [D loss: 0.329219] [G loss: 1.276771]\n",
      "[Epoch 3] [Batch 2000/3737] [D loss: -0.207140] [G loss: 0.701433]\n",
      "[Epoch 3] [Batch 2250/3737] [D loss: 0.143166] [G loss: 2.020965]\n",
      "[Epoch 3] [Batch 2500/3737] [D loss: -0.349503] [G loss: 0.797909]\n",
      "[Epoch 3] [Batch 2750/3737] [D loss: -0.221990] [G loss: 1.547911]\n",
      "[Epoch 3] [Batch 3000/3737] [D loss: -0.086374] [G loss: 1.143537]\n",
      "[Epoch 3] [Batch 3250/3737] [D loss: -0.088494] [G loss: 1.313278]\n",
      "[Epoch 3] [Batch 3500/3737] [D loss: 0.234428] [G loss: -0.635801]\n",
      "[Epoch 4] [Batch 0/3737] [D loss: -0.275380] [G loss: 0.452876]\n",
      "[Epoch 4] [Batch 250/3737] [D loss: 0.356275] [G loss: 1.430155]\n",
      "[Epoch 4] [Batch 500/3737] [D loss: -0.223136] [G loss: 1.634903]\n",
      "[Epoch 4] [Batch 750/3737] [D loss: -0.351857] [G loss: 0.662438]\n",
      "[Epoch 4] [Batch 1000/3737] [D loss: -0.270997] [G loss: 0.686264]\n",
      "[Epoch 4] [Batch 1250/3737] [D loss: -0.007214] [G loss: 0.842645]\n",
      "[Epoch 4] [Batch 1500/3737] [D loss: -0.099852] [G loss: 0.235427]\n",
      "[Epoch 4] [Batch 1750/3737] [D loss: -0.386831] [G loss: 1.667741]\n",
      "[Epoch 4] [Batch 2000/3737] [D loss: -0.211305] [G loss: 1.300340]\n",
      "[Epoch 4] [Batch 2250/3737] [D loss: 0.073939] [G loss: 1.168992]\n",
      "[Epoch 4] [Batch 2500/3737] [D loss: 0.237863] [G loss: 0.601869]\n",
      "[Epoch 4] [Batch 2750/3737] [D loss: -0.231474] [G loss: 0.398028]\n",
      "[Epoch 4] [Batch 3000/3737] [D loss: -0.038115] [G loss: 1.386296]\n",
      "[Epoch 4] [Batch 3250/3737] [D loss: -0.770206] [G loss: 0.398781]\n",
      "[Epoch 4] [Batch 3500/3737] [D loss: -0.913079] [G loss: 0.512146]\n",
      "[Epoch 5] [Batch 0/3737] [D loss: 0.094992] [G loss: 0.541725]\n",
      "[Epoch 5] [Batch 250/3737] [D loss: -0.133010] [G loss: -0.235967]\n",
      "[Epoch 5] [Batch 500/3737] [D loss: -0.327013] [G loss: 0.058998]\n",
      "[Epoch 5] [Batch 750/3737] [D loss: -0.338666] [G loss: 0.614424]\n",
      "[Epoch 5] [Batch 1000/3737] [D loss: -0.592323] [G loss: -0.078256]\n",
      "[Epoch 5] [Batch 1250/3737] [D loss: -0.164569] [G loss: 0.143266]\n",
      "[Epoch 5] [Batch 1500/3737] [D loss: -0.443706] [G loss: 0.783923]\n",
      "[Epoch 5] [Batch 1750/3737] [D loss: -0.432519] [G loss: 0.962415]\n",
      "[Epoch 5] [Batch 2000/3737] [D loss: -0.114462] [G loss: 0.100344]\n",
      "[Epoch 5] [Batch 2250/3737] [D loss: -0.612403] [G loss: 0.071551]\n",
      "[Epoch 5] [Batch 2500/3737] [D loss: -0.439647] [G loss: 0.262306]\n",
      "[Epoch 5] [Batch 2750/3737] [D loss: -0.082147] [G loss: 0.525498]\n",
      "[Epoch 5] [Batch 3000/3737] [D loss: -0.044328] [G loss: -0.036940]\n",
      "[Epoch 5] [Batch 3250/3737] [D loss: -0.232136] [G loss: -0.051953]\n",
      "[Epoch 5] [Batch 3500/3737] [D loss: -0.207020] [G loss: -0.103549]\n",
      "[Epoch 6] [Batch 0/3737] [D loss: -0.410542] [G loss: 0.660814]\n",
      "[Epoch 6] [Batch 250/3737] [D loss: -0.397648] [G loss: 0.441316]\n",
      "[Epoch 6] [Batch 500/3737] [D loss: -0.128694] [G loss: 0.906605]\n",
      "[Epoch 6] [Batch 750/3737] [D loss: -0.257804] [G loss: 0.226878]\n",
      "[Epoch 6] [Batch 1000/3737] [D loss: -0.508607] [G loss: -0.296375]\n",
      "[Epoch 6] [Batch 1250/3737] [D loss: -0.538714] [G loss: 0.734761]\n",
      "[Epoch 6] [Batch 1500/3737] [D loss: -0.477389] [G loss: -0.065835]\n",
      "[Epoch 6] [Batch 1750/3737] [D loss: 0.012796] [G loss: 0.232045]\n",
      "[Epoch 6] [Batch 2000/3737] [D loss: -0.421205] [G loss: -0.109580]\n",
      "[Epoch 6] [Batch 2250/3737] [D loss: 0.060245] [G loss: -0.496551]\n",
      "[Epoch 6] [Batch 2500/3737] [D loss: -0.241274] [G loss: 0.436664]\n",
      "[Epoch 6] [Batch 2750/3737] [D loss: -0.467700] [G loss: -0.061704]\n",
      "[Epoch 6] [Batch 3000/3737] [D loss: -0.328282] [G loss: 0.030636]\n",
      "[Epoch 6] [Batch 3250/3737] [D loss: -0.414064] [G loss: -0.328220]\n",
      "[Epoch 6] [Batch 3500/3737] [D loss: 0.031562] [G loss: -0.233620]\n",
      "[Epoch 7] [Batch 0/3737] [D loss: -0.528602] [G loss: 0.348143]\n",
      "[Epoch 7] [Batch 250/3737] [D loss: -0.209373] [G loss: 0.158986]\n",
      "[Epoch 7] [Batch 500/3737] [D loss: -0.567541] [G loss: -0.542774]\n",
      "[Epoch 7] [Batch 750/3737] [D loss: -0.431975] [G loss: -0.164377]\n",
      "[Epoch 7] [Batch 1000/3737] [D loss: -0.400243] [G loss: 0.387267]\n",
      "[Epoch 7] [Batch 1250/3737] [D loss: -0.176985] [G loss: 0.287792]\n",
      "[Epoch 7] [Batch 1500/3737] [D loss: -0.609461] [G loss: -0.203086]\n",
      "[Epoch 7] [Batch 1750/3737] [D loss: -0.182845] [G loss: -0.047617]\n",
      "[Epoch 7] [Batch 2000/3737] [D loss: -0.380829] [G loss: -0.537117]\n",
      "[Epoch 7] [Batch 2250/3737] [D loss: -0.580121] [G loss: -0.504850]\n",
      "[Epoch 7] [Batch 2500/3737] [D loss: -0.085229] [G loss: 0.186901]\n",
      "[Epoch 7] [Batch 2750/3737] [D loss: -0.192393] [G loss: -0.032533]\n",
      "[Epoch 7] [Batch 3000/3737] [D loss: -0.031081] [G loss: -0.154323]\n",
      "[Epoch 7] [Batch 3250/3737] [D loss: -0.363739] [G loss: 0.140496]\n",
      "[Epoch 7] [Batch 3500/3737] [D loss: -0.315853] [G loss: -0.127892]\n",
      "[Epoch 8] [Batch 0/3737] [D loss: -0.698547] [G loss: -0.047313]\n",
      "[Epoch 8] [Batch 250/3737] [D loss: -0.517731] [G loss: -0.476822]\n",
      "[Epoch 8] [Batch 500/3737] [D loss: -0.570859] [G loss: 0.536279]\n",
      "[Epoch 8] [Batch 750/3737] [D loss: -0.246507] [G loss: 0.079343]\n",
      "[Epoch 8] [Batch 1000/3737] [D loss: -0.608655] [G loss: -0.309716]\n",
      "[Epoch 8] [Batch 1250/3737] [D loss: -0.710207] [G loss: -0.101989]\n",
      "[Epoch 8] [Batch 1500/3737] [D loss: 0.246794] [G loss: 0.631505]\n",
      "[Epoch 8] [Batch 1750/3737] [D loss: -0.228054] [G loss: 0.088065]\n",
      "[Epoch 8] [Batch 2000/3737] [D loss: -0.143404] [G loss: -0.755487]\n",
      "[Epoch 8] [Batch 2250/3737] [D loss: -0.399775] [G loss: 0.043060]\n",
      "[Epoch 8] [Batch 2500/3737] [D loss: -0.108065] [G loss: 0.070704]\n",
      "[Epoch 8] [Batch 2750/3737] [D loss: -0.242912] [G loss: -0.252919]\n",
      "[Epoch 8] [Batch 3000/3737] [D loss: -0.426990] [G loss: -0.367320]\n",
      "[Epoch 8] [Batch 3250/3737] [D loss: 0.014496] [G loss: -0.555660]\n",
      "[Epoch 8] [Batch 3500/3737] [D loss: -0.429246] [G loss: -0.273440]\n",
      "[Epoch 9] [Batch 0/3737] [D loss: -0.370505] [G loss: -0.408014]\n",
      "[Epoch 9] [Batch 250/3737] [D loss: -0.208076] [G loss: -0.714587]\n",
      "[Epoch 9] [Batch 500/3737] [D loss: -0.372252] [G loss: -0.141669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] [Batch 750/3737] [D loss: -0.237303] [G loss: -0.069140]\n",
      "[Epoch 9] [Batch 1000/3737] [D loss: 0.182671] [G loss: 0.062264]\n",
      "[Epoch 9] [Batch 1250/3737] [D loss: -0.038857] [G loss: 0.437486]\n",
      "[Epoch 9] [Batch 1500/3737] [D loss: 0.275319] [G loss: -0.636727]\n",
      "[Epoch 9] [Batch 1750/3737] [D loss: -0.415891] [G loss: -0.305466]\n",
      "[Epoch 9] [Batch 2000/3737] [D loss: 0.040276] [G loss: -0.177399]\n",
      "[Epoch 9] [Batch 2250/3737] [D loss: -0.120701] [G loss: -0.103230]\n",
      "[Epoch 9] [Batch 2500/3737] [D loss: -0.173978] [G loss: -0.694968]\n",
      "[Epoch 9] [Batch 2750/3737] [D loss: -0.448101] [G loss: -0.758566]\n",
      "[Epoch 9] [Batch 3000/3737] [D loss: 0.094326] [G loss: -0.505736]\n",
      "[Epoch 9] [Batch 3250/3737] [D loss: -0.462984] [G loss: -0.780729]\n",
      "[Epoch 9] [Batch 3500/3737] [D loss: -0.004895] [G loss: -0.516706]\n",
      "[Epoch 10] [Batch 0/3737] [D loss: -0.639158] [G loss: -0.427350]\n",
      "[Epoch 10] [Batch 250/3737] [D loss: -0.320375] [G loss: -0.715077]\n",
      "[Epoch 10] [Batch 500/3737] [D loss: -0.297208] [G loss: -0.598012]\n",
      "[Epoch 10] [Batch 750/3737] [D loss: -0.323688] [G loss: -0.752116]\n",
      "[Epoch 10] [Batch 1000/3737] [D loss: -0.542228] [G loss: -0.459985]\n",
      "[Epoch 10] [Batch 1250/3737] [D loss: -0.205353] [G loss: -0.423857]\n",
      "[Epoch 10] [Batch 1500/3737] [D loss: -0.171064] [G loss: -0.213876]\n",
      "[Epoch 10] [Batch 1750/3737] [D loss: -0.520832] [G loss: -0.598262]\n",
      "[Epoch 10] [Batch 2000/3737] [D loss: -0.296161] [G loss: -0.388200]\n",
      "[Epoch 10] [Batch 2250/3737] [D loss: -0.049833] [G loss: -0.559957]\n",
      "[Epoch 10] [Batch 2500/3737] [D loss: -0.443110] [G loss: -0.791599]\n",
      "[Epoch 10] [Batch 2750/3737] [D loss: -0.262940] [G loss: -0.341626]\n",
      "[Epoch 10] [Batch 3000/3737] [D loss: -0.308852] [G loss: -0.176961]\n",
      "[Epoch 10] [Batch 3250/3737] [D loss: -0.157709] [G loss: -0.698528]\n",
      "[Epoch 10] [Batch 3500/3737] [D loss: -0.318736] [G loss: -1.018228]\n",
      "[Epoch 11] [Batch 0/3737] [D loss: -0.361580] [G loss: -0.496515]\n",
      "[Epoch 11] [Batch 250/3737] [D loss: -0.512006] [G loss: -0.711638]\n",
      "[Epoch 11] [Batch 500/3737] [D loss: -0.346792] [G loss: -0.469712]\n",
      "[Epoch 11] [Batch 750/3737] [D loss: -0.201446] [G loss: -0.025112]\n",
      "[Epoch 11] [Batch 1000/3737] [D loss: -0.375591] [G loss: -0.920561]\n",
      "[Epoch 11] [Batch 1250/3737] [D loss: -0.161603] [G loss: -0.698450]\n",
      "[Epoch 11] [Batch 1500/3737] [D loss: -0.432866] [G loss: 0.278012]\n",
      "[Epoch 11] [Batch 1750/3737] [D loss: -0.398910] [G loss: -0.608236]\n",
      "[Epoch 11] [Batch 2000/3737] [D loss: -0.139869] [G loss: -0.781838]\n",
      "[Epoch 11] [Batch 2250/3737] [D loss: -0.288664] [G loss: -0.776335]\n",
      "[Epoch 11] [Batch 2500/3737] [D loss: -0.323090] [G loss: -0.922244]\n",
      "[Epoch 11] [Batch 2750/3737] [D loss: -0.493260] [G loss: -0.495107]\n",
      "[Epoch 11] [Batch 3000/3737] [D loss: -0.008799] [G loss: -0.665668]\n",
      "[Epoch 11] [Batch 3250/3737] [D loss: -0.205964] [G loss: -0.410497]\n",
      "[Epoch 11] [Batch 3500/3737] [D loss: -0.695906] [G loss: -0.020135]\n",
      "[Epoch 12] [Batch 0/3737] [D loss: -0.503932] [G loss: -1.035787]\n",
      "[Epoch 12] [Batch 250/3737] [D loss: -0.255398] [G loss: -0.724887]\n",
      "[Epoch 12] [Batch 500/3737] [D loss: -0.490007] [G loss: -0.477945]\n",
      "[Epoch 12] [Batch 750/3737] [D loss: -0.236021] [G loss: -0.818540]\n",
      "[Epoch 12] [Batch 1000/3737] [D loss: -0.311716] [G loss: -0.507697]\n",
      "[Epoch 12] [Batch 1250/3737] [D loss: -0.169270] [G loss: -1.019346]\n",
      "[Epoch 12] [Batch 1500/3737] [D loss: -0.325242] [G loss: -0.512778]\n",
      "[Epoch 12] [Batch 1750/3737] [D loss: -0.261511] [G loss: -0.765846]\n",
      "[Epoch 12] [Batch 2000/3737] [D loss: -0.457105] [G loss: -0.297738]\n",
      "[Epoch 12] [Batch 2250/3737] [D loss: -0.030299] [G loss: -0.917377]\n",
      "[Epoch 12] [Batch 2500/3737] [D loss: -0.164474] [G loss: -0.967564]\n",
      "[Epoch 12] [Batch 2750/3737] [D loss: -0.369995] [G loss: -0.408327]\n",
      "[Epoch 12] [Batch 3000/3737] [D loss: -0.091811] [G loss: -0.764633]\n",
      "[Epoch 12] [Batch 3250/3737] [D loss: -0.046003] [G loss: -0.651302]\n",
      "[Epoch 12] [Batch 3500/3737] [D loss: -0.614897] [G loss: -0.447602]\n",
      "[Epoch 13] [Batch 0/3737] [D loss: -0.344918] [G loss: -0.974486]\n",
      "[Epoch 13] [Batch 250/3737] [D loss: -0.327412] [G loss: -1.210496]\n",
      "[Epoch 13] [Batch 500/3737] [D loss: -0.067412] [G loss: -0.192577]\n",
      "[Epoch 13] [Batch 750/3737] [D loss: -0.410493] [G loss: -1.090683]\n",
      "[Epoch 13] [Batch 1000/3737] [D loss: -0.534824] [G loss: -0.792083]\n",
      "[Epoch 13] [Batch 1250/3737] [D loss: -0.582283] [G loss: -1.266785]\n",
      "[Epoch 13] [Batch 1500/3737] [D loss: -0.185690] [G loss: -0.169087]\n",
      "[Epoch 13] [Batch 1750/3737] [D loss: -0.375614] [G loss: -0.893909]\n",
      "[Epoch 13] [Batch 2000/3737] [D loss: -0.310067] [G loss: -1.021317]\n",
      "[Epoch 13] [Batch 2250/3737] [D loss: -0.420903] [G loss: -0.813495]\n",
      "[Epoch 13] [Batch 2500/3737] [D loss: -0.191293] [G loss: -1.186405]\n",
      "[Epoch 13] [Batch 2750/3737] [D loss: -0.222762] [G loss: -0.917759]\n",
      "[Epoch 13] [Batch 3000/3737] [D loss: -0.082785] [G loss: -0.699887]\n",
      "[Epoch 13] [Batch 3250/3737] [D loss: -0.228663] [G loss: -1.476383]\n",
      "[Epoch 13] [Batch 3500/3737] [D loss: -0.037632] [G loss: -0.940756]\n",
      "[Epoch 14] [Batch 0/3737] [D loss: -0.052168] [G loss: -1.139315]\n",
      "[Epoch 14] [Batch 250/3737] [D loss: -0.673331] [G loss: -1.257033]\n",
      "[Epoch 14] [Batch 500/3737] [D loss: -0.256875] [G loss: -0.991471]\n",
      "[Epoch 14] [Batch 750/3737] [D loss: -0.357572] [G loss: -1.692758]\n",
      "[Epoch 14] [Batch 1000/3737] [D loss: -0.247863] [G loss: -1.454740]\n",
      "[Epoch 14] [Batch 1250/3737] [D loss: -0.226516] [G loss: -1.196938]\n",
      "[Epoch 14] [Batch 1500/3737] [D loss: -0.364941] [G loss: -1.038771]\n",
      "[Epoch 14] [Batch 1750/3737] [D loss: 0.092403] [G loss: -1.381449]\n",
      "[Epoch 14] [Batch 2000/3737] [D loss: -0.200934] [G loss: -1.724644]\n",
      "[Epoch 14] [Batch 2250/3737] [D loss: -0.377942] [G loss: -1.154948]\n",
      "[Epoch 14] [Batch 2500/3737] [D loss: -0.054508] [G loss: -1.242553]\n",
      "[Epoch 14] [Batch 2750/3737] [D loss: -0.150127] [G loss: -1.480252]\n",
      "[Epoch 14] [Batch 3000/3737] [D loss: -0.159022] [G loss: -0.927511]\n",
      "[Epoch 14] [Batch 3250/3737] [D loss: -0.207758] [G loss: -0.357061]\n",
      "[Epoch 14] [Batch 3500/3737] [D loss: -0.227287] [G loss: -1.795383]\n",
      "[Epoch 15] [Batch 0/3737] [D loss: -0.575707] [G loss: -0.605872]\n",
      "[Epoch 15] [Batch 250/3737] [D loss: -0.577826] [G loss: -1.239187]\n",
      "[Epoch 15] [Batch 500/3737] [D loss: -0.188088] [G loss: -0.775771]\n",
      "[Epoch 15] [Batch 750/3737] [D loss: -0.286819] [G loss: -1.417017]\n",
      "[Epoch 15] [Batch 1000/3737] [D loss: -0.224625] [G loss: -1.414507]\n",
      "[Epoch 15] [Batch 1250/3737] [D loss: -0.123981] [G loss: -1.015811]\n",
      "[Epoch 15] [Batch 1500/3737] [D loss: -0.478198] [G loss: -1.000355]\n",
      "[Epoch 15] [Batch 1750/3737] [D loss: -0.402276] [G loss: -0.891334]\n",
      "[Epoch 15] [Batch 2000/3737] [D loss: -0.593210] [G loss: -1.387449]\n",
      "[Epoch 15] [Batch 2250/3737] [D loss: -0.457187] [G loss: -1.286941]\n",
      "[Epoch 15] [Batch 2500/3737] [D loss: -0.339417] [G loss: -1.557802]\n",
      "[Epoch 15] [Batch 2750/3737] [D loss: -0.364531] [G loss: -1.244572]\n",
      "[Epoch 15] [Batch 3000/3737] [D loss: -0.069249] [G loss: -1.375891]\n",
      "[Epoch 15] [Batch 3250/3737] [D loss: -0.324828] [G loss: -1.020709]\n",
      "[Epoch 15] [Batch 3500/3737] [D loss: -0.310243] [G loss: -1.495239]\n",
      "[Epoch 16] [Batch 0/3737] [D loss: 0.149505] [G loss: -0.845634]\n",
      "[Epoch 16] [Batch 250/3737] [D loss: 0.073837] [G loss: -1.353309]\n",
      "[Epoch 16] [Batch 500/3737] [D loss: -0.016802] [G loss: -1.105230]\n",
      "[Epoch 16] [Batch 750/3737] [D loss: -0.574752] [G loss: -1.113006]\n",
      "[Epoch 16] [Batch 1000/3737] [D loss: -0.098829] [G loss: -1.410525]\n",
      "[Epoch 16] [Batch 1250/3737] [D loss: -0.312921] [G loss: -1.699721]\n",
      "[Epoch 16] [Batch 1500/3737] [D loss: -0.449572] [G loss: -1.176762]\n",
      "[Epoch 16] [Batch 1750/3737] [D loss: 0.080495] [G loss: -1.477812]\n",
      "[Epoch 16] [Batch 2000/3737] [D loss: -0.486719] [G loss: -1.366646]\n",
      "[Epoch 16] [Batch 2250/3737] [D loss: -0.497816] [G loss: -1.145777]\n",
      "[Epoch 16] [Batch 2500/3737] [D loss: -0.477077] [G loss: -1.937519]\n",
      "[Epoch 16] [Batch 2750/3737] [D loss: -0.335177] [G loss: -1.411165]\n",
      "[Epoch 16] [Batch 3000/3737] [D loss: -0.181468] [G loss: -2.108010]\n",
      "[Epoch 16] [Batch 3250/3737] [D loss: -0.223187] [G loss: -1.696456]\n",
      "[Epoch 16] [Batch 3500/3737] [D loss: -0.316191] [G loss: -1.719914]\n",
      "[Epoch 17] [Batch 0/3737] [D loss: -0.162497] [G loss: -1.418906]\n",
      "[Epoch 17] [Batch 250/3737] [D loss: -0.412654] [G loss: -1.359848]\n",
      "[Epoch 17] [Batch 500/3737] [D loss: -0.702768] [G loss: -1.261707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] [Batch 750/3737] [D loss: -0.328045] [G loss: -1.581429]\n",
      "[Epoch 17] [Batch 1000/3737] [D loss: -0.474953] [G loss: -1.531839]\n",
      "[Epoch 17] [Batch 1250/3737] [D loss: -0.156620] [G loss: -1.944632]\n",
      "[Epoch 17] [Batch 1500/3737] [D loss: -0.133209] [G loss: -1.206184]\n",
      "[Epoch 17] [Batch 1750/3737] [D loss: -0.209734] [G loss: -0.783514]\n",
      "[Epoch 17] [Batch 2000/3737] [D loss: -0.230988] [G loss: -1.435113]\n",
      "[Epoch 17] [Batch 2250/3737] [D loss: -0.303032] [G loss: -1.303110]\n",
      "[Epoch 17] [Batch 2500/3737] [D loss: -0.109417] [G loss: -1.085255]\n",
      "[Epoch 17] [Batch 2750/3737] [D loss: 0.200756] [G loss: -1.100820]\n",
      "[Epoch 17] [Batch 3000/3737] [D loss: -0.258099] [G loss: -1.577160]\n",
      "[Epoch 17] [Batch 3250/3737] [D loss: -0.181556] [G loss: -1.338347]\n",
      "[Epoch 17] [Batch 3500/3737] [D loss: -0.148168] [G loss: -1.479494]\n",
      "[Epoch 18] [Batch 0/3737] [D loss: -0.089086] [G loss: -1.458939]\n",
      "[Epoch 18] [Batch 250/3737] [D loss: -0.498616] [G loss: -1.263058]\n",
      "[Epoch 18] [Batch 500/3737] [D loss: -0.548386] [G loss: -0.990208]\n",
      "[Epoch 18] [Batch 750/3737] [D loss: 0.049356] [G loss: -1.237124]\n",
      "[Epoch 18] [Batch 1000/3737] [D loss: -0.177824] [G loss: -1.318271]\n",
      "[Epoch 18] [Batch 1250/3737] [D loss: -0.271002] [G loss: -1.291337]\n",
      "[Epoch 18] [Batch 1500/3737] [D loss: -0.502849] [G loss: -0.851563]\n",
      "[Epoch 18] [Batch 1750/3737] [D loss: -0.181496] [G loss: -1.537686]\n",
      "[Epoch 18] [Batch 2000/3737] [D loss: -0.366813] [G loss: -1.537959]\n",
      "[Epoch 18] [Batch 2250/3737] [D loss: -0.356604] [G loss: -1.857931]\n",
      "[Epoch 18] [Batch 2500/3737] [D loss: 0.054740] [G loss: -1.321179]\n",
      "[Epoch 18] [Batch 2750/3737] [D loss: -0.198266] [G loss: -1.482907]\n",
      "[Epoch 18] [Batch 3000/3737] [D loss: -0.242350] [G loss: -1.802581]\n",
      "[Epoch 18] [Batch 3250/3737] [D loss: -0.349443] [G loss: -1.515031]\n",
      "[Epoch 18] [Batch 3500/3737] [D loss: -0.152309] [G loss: -0.657092]\n",
      "[Epoch 19] [Batch 0/3737] [D loss: -0.029962] [G loss: -1.925011]\n",
      "[Epoch 19] [Batch 250/3737] [D loss: -0.496303] [G loss: -1.753162]\n",
      "[Epoch 19] [Batch 500/3737] [D loss: -0.536414] [G loss: -1.442116]\n",
      "[Epoch 19] [Batch 750/3737] [D loss: -0.217100] [G loss: -1.083218]\n",
      "[Epoch 19] [Batch 1000/3737] [D loss: -0.379074] [G loss: -1.405227]\n",
      "[Epoch 19] [Batch 1250/3737] [D loss: -0.395984] [G loss: -1.014590]\n",
      "[Epoch 19] [Batch 1500/3737] [D loss: -0.024433] [G loss: -1.202111]\n",
      "[Epoch 19] [Batch 1750/3737] [D loss: 0.201570] [G loss: -1.419394]\n",
      "[Epoch 19] [Batch 2000/3737] [D loss: -0.162805] [G loss: -1.829865]\n",
      "[Epoch 19] [Batch 2250/3737] [D loss: -0.684524] [G loss: -1.538449]\n",
      "[Epoch 19] [Batch 2500/3737] [D loss: 0.145024] [G loss: -1.647954]\n",
      "[Epoch 19] [Batch 2750/3737] [D loss: -0.413098] [G loss: -1.669558]\n",
      "[Epoch 19] [Batch 3000/3737] [D loss: -0.459239] [G loss: -1.772841]\n",
      "[Epoch 19] [Batch 3250/3737] [D loss: -0.178326] [G loss: -1.698172]\n",
      "[Epoch 19] [Batch 3500/3737] [D loss: -0.231154] [G loss: -2.070004]\n",
      "[Epoch 20] [Batch 0/3737] [D loss: -0.547928] [G loss: -1.672979]\n",
      "[Epoch 20] [Batch 250/3737] [D loss: -0.478515] [G loss: -1.654760]\n",
      "[Epoch 20] [Batch 500/3737] [D loss: -0.142386] [G loss: -1.178778]\n",
      "[Epoch 20] [Batch 750/3737] [D loss: -0.775349] [G loss: -1.509599]\n",
      "[Epoch 20] [Batch 1000/3737] [D loss: -0.362655] [G loss: -1.755226]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     lr_schedulers \u001b[38;5;241m=\u001b[39m (gen_scheduler, dis_scheduler) \u001b[38;5;28;01mif\u001b[39;00m lr_decay \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     train(noise, generator, discriminator, optim_gen, optim_dis,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#           epoch, writer, lr_schedulers, img_size=32, latent_dim=latent_dim,\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#           n_critic=n_critic, gener_batch_size=gener_batch_size)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtrain_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_loader_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_loader_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_dis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedulers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mn_critic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgener_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgener_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_fid\u001b[39m\u001b[38;5;124m'\u001b[39m: best}\n\u001b[0;32m     14\u001b[0m     checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtrain_v2\u001b[1;34m(gen_dataloader, disc_dataloader, generator, discriminator, optim_gen, optim_dis, epoch, writer, schedulers, img_size, latent_dim, n_critic, gener_batch_size, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m         real_valid \u001b[38;5;241m=\u001b[39m discriminator(real_imgs)\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# obtenemos un batch de imágenes fake generadas por el\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# modelo generador sin entrenamiento\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m         fake_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_imgs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#         print(fake_imgs.detach().shape)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# Hallamos la predicción del discriminador\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         fake_valid \u001b[38;5;241m=\u001b[39m discriminator(fake_imgs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mGenerator_2.forward\u001b[1;34m(self, noise)\u001b[0m\n\u001b[0;32m     68\u001b[0m x, H, W \u001b[38;5;241m=\u001b[39m UpSampling(x, H, W)\n\u001b[0;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding_2\n\u001b[1;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformerEncoder_encoder2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m x, H, W \u001b[38;5;241m=\u001b[39m UpSampling(x, H, W)\n\u001b[0;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding_3\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m Encoder_Block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoder_Blocks:\n\u001b[1;32m---> 10\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder_Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mEncoder_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x2)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m attn \u001b[38;5;241m=\u001b[39m dot\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout(attn)\n\u001b[1;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(b, n, c)\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):\n",
    "\n",
    "    lr_schedulers = (gen_scheduler, dis_scheduler) if lr_decay else None\n",
    "\n",
    "#     train(noise, generator, discriminator, optim_gen, optim_dis,\n",
    "#           epoch, writer, lr_schedulers, img_size=32, latent_dim=latent_dim,\n",
    "#           n_critic=n_critic, gener_batch_size=gener_batch_size)\n",
    "    \n",
    "    train_v2(dt_loader_gen, dt_loader_real, generator, discriminator, optim_gen, optim_dis,\n",
    "          epoch, writer, lr_schedulers, img_size=32, latent_dim=latent_dim,\n",
    "          n_critic=n_critic, gener_batch_size=gener_batch_size)\n",
    "\n",
    "    checkpoint = {'epoch': epoch, 'best_fid': best}\n",
    "    checkpoint['generator_state_dict'] = generator.state_dict()\n",
    "    checkpoint['discriminator_state_dict'] = discriminator.state_dict()\n",
    "    save_checkpoint(checkpoint, is_best=True, output_dir=output_dir)\n",
    "#     score = validate(generator, writer_dict, fid_stat)\n",
    "\n",
    "#     print(f'FID score: {score} - best ID score: {best} || @ epoch {epoch+1}.')\n",
    "#     if epoch == 0 or epoch > 30:\n",
    "#         if score < best:\n",
    "#             \n",
    "#             best = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0fab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3668a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4863a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753c9ffa",
   "metadata": {},
   "source": [
    "# TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d645ab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T04:06:35.748455Z",
     "start_time": "2022-08-08T04:06:35.748455Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_pixeled = './Test_images/pixeled/'\n",
    "\n",
    "# Tamaño que deseamos que tengan las imágenes\n",
    "image_size = 32\n",
    "# Tamaño del lote de imágenes\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "pixeled_imgs_test = dtst.ImageFolder(\n",
    "    root=data_folder_pixeled,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # Esta desviación estandar y media es la hallada para los datos de\n",
    "        # entrenamiento\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "\n",
    "\n",
    "dt_loader_gen_test = DataLoader(pixeled_imgs_test,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=1,\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294d6f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T04:06:35.750458Z",
     "start_time": "2022-08-08T04:06:35.750458Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (img, _) in enumerate(dt_loader_gen_test):   \n",
    "       \n",
    "    fake_imgs = img.view(16,3072).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    generated_imgs = generator(fake_imgs)\n",
    "    \n",
    "    show_tensor_images(generated_imgs,16,size=(3,32,32))\n",
    "    sample_imgs = generated_imgs[:16]\n",
    "#     img_grid = make_grid(sample_imgs, nrow=4,\n",
    "#     normalize=True, scale_each=True)\n",
    "    save_image(sample_imgs, f'./generated_imgs_test/test_img_{index % len(dt_loader_gen_test)}.jpg', nrow=4, normalize=True, scale_each=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ab91d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T04:06:35.752455Z",
     "start_time": "2022-08-08T04:06:35.752455Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_complete = './Test_images/original/'\n",
    "\n",
    "complete_imgs_test = dtst.ImageFolder(\n",
    "    root=data_folder_complete,\n",
    "    transform=transforms.Compose([\n",
    "        # Se usa el resize en caso no todas las imágenes de \n",
    "        # entrada tengan el tamaño de 32px\n",
    "        transforms.Resize(image_size),\n",
    "        # CenterCrop busca recortar la imagen en caso sea muy grande al tamaño dado\n",
    "        transforms.CenterCrop(image_size),\n",
    "        # ToTensor convierte finalmente la imagen a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize permite la normalización de la información\n",
    "        # El problema encontrado es que necesitamos hallar la desviación estandar\n",
    "        # media de toda la información para realizar una correcta normalización\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "dt_loader_real_test = DataLoader(complete_imgs_test,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False,\n",
    "                       num_workers=1,\n",
    "                       drop_last=True)\n",
    "\n",
    "for index, (img, _) in enumerate(dt_loader_real_test):   \n",
    "       \n",
    "    real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    show_tensor_images(real_imgs,16,size=(3,32,32))\n",
    "    sample_imgs = real_imgs[:16]\n",
    "\n",
    "    save_image(sample_imgs, f'./generated_imgs_test/real2/test_img_{index % len(dt_loader_real_test)}.png', nrow=4, normalize=True, scale_each=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d23ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "324px",
    "width": "357px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.844px",
    "left": "596px",
    "right": "20px",
    "top": "136px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
